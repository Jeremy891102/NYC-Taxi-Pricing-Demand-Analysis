{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHVHV Trip Data 2024 - Demand Analysis\n",
    "\n",
    "\n",
    "## Analysis Workflow\n",
    "\n",
    "1. **Dataset Overview** \n",
    "2. **Data Quality Assessment** \n",
    "3. **Data Cleaning** \n",
    "4. **Summary Statistics**  \n",
    "5. **EDA** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Environment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import json\n",
    "import warnings\n",
    "from glob import glob\n",
    "import gc\n",
    "from IPython.display import display\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Pandas settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 30)\n",
    "\n",
    "# Matplotlib settings\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['savefig.facecolor'] = 'white'\n",
    "plt.rcParams['savefig.edgecolor'] = 'none'\n",
    "plt.rcParams['text.color'] = 'black'\n",
    "plt.rcParams['axes.labelcolor'] = 'black'\n",
    "plt.rcParams['xtick.color'] = 'black'\n",
    "plt.rcParams['ytick.color'] = 'black'\n",
    "plt.rcParams['axes.edgecolor'] = 'black'\n",
    "plt.rcParams['axes.titlecolor'] = 'black'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Functions\n",
    "\n",
    "Define analysis functions that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_statistics():\n",
    "    \"\"\"\n",
    "    Get basic statistics about all parquet files in the current directory.\n",
    "    \n",
    "    Scans the current directory for FHVHV trip data parquet files and calculates\n",
    "    total file count and total size.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of Path objects for all found parquet files\n",
    "        \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> files = get_file_statistics()\n",
    "    Dataset: 12 files, 5.40 GB total\n",
    "    \"\"\"\n",
    "    data_dir = Path(\".\")\n",
    "    parquet_files = sorted(list(data_dir.glob(\"fhvhv_tripdata_2024-*.parquet\")))\n",
    "    parquet_files = [f for f in parquet_files if f.name != \"fhvhv_tripdata_2024.parquet\"]\n",
    "    \n",
    "    total_size = sum(file.stat().st_size / (1024**3) for file in parquet_files)\n",
    "    print(f\"Dataset: {len(parquet_files)} files, {total_size:.6f} GB total\")\n",
    "    return parquet_files\n",
    "\n",
    "\n",
    "def analyze_dataset_structure(df):\n",
    "    \"\"\"\n",
    "    Display basic dataset structure and schema information.\n",
    "    \n",
    "    Shows the number of rows and columns in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame to analyze\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        The input DataFrame (unchanged)\n",
    "    \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> analyze_dataset_structure(df)\n",
    "    Dataset: 43,680,321 rows × 11 columns\n",
    "    \"\"\"\n",
    "    print(f\"Dataset: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ============================================================================\n",
    "# Data Quality Analysis Functions\n",
    "def analyze_data_quality(df):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality assessment with detailed reporting.\n",
    "    \n",
    "    Analyzes the dataset for various data quality issues including:\n",
    "    - Missing values (null/NaN) - detailed by column\n",
    "    - Duplicate records\n",
    "    - Invalid values (negative fares, negative distances, negative times)\n",
    "    - Data consistency issues (dropoff before pickup)\n",
    "    - Zero values in critical fields\n",
    "    - Extreme outliers\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing trip data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        The input DataFrame (unchanged)\n",
    "    \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> analyze_data_quality(df)\n",
    "    Missing: 0 (100.00%)\n",
    "    Duplicates: 0 (0.00%)\n",
    "    Issues: Negative fares\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    total_cols = len(df.columns)\n",
    "    \n",
    "    print(f\"\\nDataset Overview:\")\n",
    "    print(f\"  Total records: {total_rows:,}\")\n",
    "    print(f\"  Total columns: {total_cols}\")\n",
    "    \n",
    "    # ===== 1. Missing Values Analysis =====\n",
    "    print(f\"\\n[1] Missing Values Analysis\")\n",
    "    print(\"-\" * 80)\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / total_rows * 100).round(2)\n",
    "    completeness = ((total_rows * total_cols - missing.sum()) / (total_rows * total_cols) * 100)\n",
    "    \n",
    "    if missing.sum() > 0:\n",
    "        print(f\"  Total missing values: {missing.sum():,} ({100-completeness:.6f}% of all cells)\")\n",
    "        print(f\"  Columns with missing values:\")\n",
    "        for col in missing[missing > 0].index:\n",
    "            print(f\"    - {col}: {missing[col]:,} ({missing_pct[col]:.6f}%)\")\n",
    "    else:\n",
    "        print(f\"  [OK] No missing values found (100% completeness)\")\n",
    "    \n",
    "    # ===== 2. Duplicate Records Analysis =====\n",
    "    print(f\"\\n[2] Duplicate Records Analysis\")\n",
    "    print(\"-\" * 80)\n",
    "    full_duplicates = df.duplicated().sum()\n",
    "    if full_duplicates > 0:\n",
    "        print(f\"  Total duplicate rows: {full_duplicates:,} ({full_duplicates/total_rows*100:.6f}%)\")\n",
    "        # Show sample duplicates\n",
    "        dup_sample = df[df.duplicated()].head(3)\n",
    "        if len(dup_sample) > 0:\n",
    "            print(f\"  Sample duplicate records:\")\n",
    "            display(dup_sample)\n",
    "    else:\n",
    "        print(f\"  [OK] No duplicate rows found\")\n",
    "    \n",
    "    # ===== 3. Negative Values Analysis =====\n",
    "    print(f\"\\n[3] Negative Values Analysis\")\n",
    "    print(\"-\" * 80)\n",
    "    negative_issues = []\n",
    "    \n",
    "    # Check fare columns\n",
    "    fare_cols = ['base_passenger_fare', 'tips', 'tolls', 'airport_fee', 'driver_pay']\n",
    "    for col in fare_cols:\n",
    "        if col in df.columns:\n",
    "            negative_count = (df[col] < 0).sum()\n",
    "            if negative_count > 0:\n",
    "                negative_pct = (negative_count / total_rows * 100)\n",
    "                min_val = df[col].min()\n",
    "                print(f\"  {col}: {negative_count:,} negative values ({negative_pct:.6f}%)\")\n",
    "                print(f\"    Min value: ${min_val:.6f}\")\n",
    "                negative_issues.append(f\"{col}: {negative_count:,}\")\n",
    "    \n",
    "    # Check trip_miles\n",
    "    if 'trip_miles' in df.columns:\n",
    "        negative_miles = (df['trip_miles'] < 0).sum()\n",
    "        if negative_miles > 0:\n",
    "            negative_pct = (negative_miles / total_rows * 100)\n",
    "            min_val = df['trip_miles'].min()\n",
    "            print(f\"  trip_miles: {negative_miles:,} negative values ({negative_pct:.6f}%)\")\n",
    "            print(f\"    Min value: {min_val:.6f} miles\")\n",
    "            negative_issues.append(f\"trip_miles: {negative_miles:,}\")\n",
    "    \n",
    "    # Check trip_time\n",
    "    if 'trip_time' in df.columns:\n",
    "        negative_time = (df['trip_time'] < 0).sum()\n",
    "        if negative_time > 0:\n",
    "            negative_pct = (negative_time / total_rows * 100)\n",
    "            min_val = df['trip_time'].min()\n",
    "            print(f\"  trip_time: {negative_time:,} negative values ({negative_pct:.6f}%)\")\n",
    "            print(f\"    Min value: {min_val} seconds\")\n",
    "            negative_issues.append(f\"trip_time: {negative_time:,}\")\n",
    "    \n",
    "    if not negative_issues:\n",
    "        print(f\"  [OK] No negative values found\")\n",
    "    \n",
    "    # ===== 4. Zero Values Analysis =====\n",
    "    print(f\"\\n[4] Zero Values Analysis (Potential Issues)\")\n",
    "    print(\"-\" * 80)\n",
    "    zero_issues = []\n",
    "    \n",
    "    if 'trip_miles' in df.columns:\n",
    "        zero_miles = (df['trip_miles'] == 0).sum()\n",
    "        if zero_miles > 0:\n",
    "            zero_pct = (zero_miles / total_rows * 100)\n",
    "            print(f\"  trip_miles: {zero_miles:,} zero values ({zero_pct:.6f}%)\")\n",
    "            zero_issues.append(f\"trip_miles: {zero_miles:,}\")\n",
    "    \n",
    "    if 'trip_time' in df.columns:\n",
    "        zero_time = (df['trip_time'] == 0).sum()\n",
    "        if zero_time > 0:\n",
    "            zero_pct = (zero_time / total_rows * 100)\n",
    "            print(f\"  trip_time: {zero_time:,} zero values ({zero_pct:.6f}%)\")\n",
    "            zero_issues.append(f\"trip_time: {zero_time:,}\")\n",
    "    \n",
    "    if 'base_passenger_fare' in df.columns:\n",
    "        zero_fare = (df['base_passenger_fare'] == 0).sum()\n",
    "        if zero_fare > 0:\n",
    "            zero_pct = (zero_fare / total_rows * 100)\n",
    "            print(f\"  base_passenger_fare: {zero_fare:,} zero values ({zero_pct:.6f}%)\")\n",
    "            zero_issues.append(f\"base_passenger_fare: {zero_fare:,}\")\n",
    "    \n",
    "    if not zero_issues:\n",
    "        print(f\"  [OK] No suspicious zero values found\")\n",
    "    \n",
    "    # ===== 5. Time Consistency Analysis =====\n",
    "    print(f\"\\n[5] Time Consistency Analysis\")\n",
    "    print(\"-\" * 80)\n",
    "    if 'pickup_datetime' in df.columns and 'dropoff_datetime' in df.columns:\n",
    "        if df['pickup_datetime'].dtype == 'datetime64[ns]' and df['dropoff_datetime'].dtype == 'datetime64[ns]':\n",
    "            invalid_order = (df['dropoff_datetime'] < df['pickup_datetime']).sum()\n",
    "            if invalid_order > 0:\n",
    "                invalid_pct = (invalid_order / total_rows * 100)\n",
    "                print(f\"  Invalid time order: {invalid_order:,} trips ({invalid_pct:.6f}%)\")\n",
    "                print(f\"    (dropoff_datetime < pickup_datetime)\")\n",
    "                \n",
    "                # Show sample invalid records\n",
    "                invalid_sample = df[df['dropoff_datetime'] < df['pickup_datetime']].head(3)\n",
    "                if len(invalid_sample) > 0:\n",
    "                    print(f\"  Sample invalid records:\")\n",
    "                    display(invalid_sample[['pickup_datetime', 'dropoff_datetime']])\n",
    "            else:\n",
    "                print(f\"  [OK] All trips have valid time order\")\n",
    "        else:\n",
    "            print(f\"  [WARN] Datetime columns not properly converted\")\n",
    "    else:\n",
    "        print(f\"  [WARN] Datetime columns not found\")\n",
    "    \n",
    "    # ===== 6. Extreme Outliers Analysis =====\n",
    "    print(f\"\\n[6] Extreme Outliers Analysis\")\n",
    "    print(\"-\" * 80)\n",
    "    outlier_issues = []\n",
    "    \n",
    "    if 'trip_miles' in df.columns:\n",
    "        # Trips longer than 100 miles (likely errors)\n",
    "        extreme_miles = (df['trip_miles'] > 100).sum()\n",
    "        if extreme_miles > 0:\n",
    "            extreme_pct = (extreme_miles / total_rows * 100)\n",
    "            max_miles = df['trip_miles'].max()\n",
    "            print(f\"  trip_miles > 100 miles: {extreme_miles:,} trips ({extreme_pct:.6f}%)\")\n",
    "            print(f\"    Max value: {max_miles:.6f} miles\")\n",
    "            outlier_issues.append(f\"trip_miles > 100: {extreme_miles:,}\")\n",
    "    \n",
    "    if 'trip_time' in df.columns:\n",
    "        # Trips longer than 24 hours (likely errors)\n",
    "        extreme_time = (df['trip_time'] > 24 * 3600).sum()\n",
    "        if extreme_time > 0:\n",
    "            extreme_pct = (extreme_time / total_rows * 100)\n",
    "            max_time = df['trip_time'].max()\n",
    "            max_hours = max_time / 3600\n",
    "            print(f\"  trip_time > 24 hours: {extreme_time:,} trips ({extreme_pct:.6f}%)\")\n",
    "            print(f\"    Max value: {max_hours:.6f} hours ({max_time:,} seconds)\")\n",
    "            outlier_issues.append(f\"trip_time > 24h: {extreme_time:,}\")\n",
    "    \n",
    "    if 'base_passenger_fare' in df.columns:\n",
    "        # Extremely high fares (likely errors)\n",
    "        extreme_fare = (df['base_passenger_fare'] > 1000).sum()\n",
    "        if extreme_fare > 0:\n",
    "            extreme_pct = (extreme_fare / total_rows * 100)\n",
    "            max_fare = df['base_passenger_fare'].max()\n",
    "            print(f\"  base_passenger_fare > $1000: {extreme_fare:,} trips ({extreme_pct:.6f}%)\")\n",
    "            print(f\"    Max value: ${max_fare:.6f}\")\n",
    "            outlier_issues.append(f\"base_passenger_fare > $1000: {extreme_fare:,}\")\n",
    "    \n",
    "    if not outlier_issues:\n",
    "        print(f\"  [OK] No extreme outliers detected\")\n",
    "    \n",
    "    # Note: Summary will be generated after all checks are complete in the main cell\n",
    "    # Return df for consistency\n",
    "    return df\n",
    "\n",
    "\n",
    "def analyze_numeric_fields(df):\n",
    "    \"\"\"\n",
    "    Analyze numeric fields in the dataset.\n",
    "    \n",
    "    Identifies numeric columns (excluding location IDs and trip_time) and\n",
    "    displays summary statistics (mean and median) for all fields.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame to analyze\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        The input DataFrame (unchanged)\n",
    "    \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> analyze_numeric_fields(df)\n",
    "    Numeric fields (8):\n",
    "      trip_miles: mean=5.08, median=3.02\n",
    "      base_passenger_fare: mean=26.06, median=19.53\n",
    "      ...\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['PULocationID', 'DOLocationID', 'trip_time']]\n",
    "    \n",
    "    print(f\"\\nNumeric fields ({len(numeric_cols)}):\")\n",
    "    for col in numeric_cols:\n",
    "        mean_val = df[col].mean()\n",
    "        median_val = df[col].median()\n",
    "        print(f\"  {col}: mean={mean_val:.6f}, median={median_val:.6f}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def analyze_categorical_fields(df):\n",
    "    \"\"\"\n",
    "    Analyze categorical fields in the dataset.\n",
    "    \n",
    "    Identifies categorical/object type columns (excluding datetime-like columns)\n",
    "    and displays the count of categorical fields.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame to analyze\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Prints the number of categorical fields\n",
    "    \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> analyze_categorical_fields(df)\n",
    "    Categorical fields: 1\n",
    "    \"\"\"\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    datetime_like = ['request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']\n",
    "    categorical_cols = [col for col in categorical_cols if col not in datetime_like]\n",
    "    if categorical_cols:\n",
    "        print(f\"Categorical fields: {len(categorical_cols)}\")\n",
    "    else:\n",
    "        print(\"No categorical fields\")\n",
    "\n",
    "\n",
    "def analyze_temporal_data(df):\n",
    "    \"\"\"\n",
    "    Analyze temporal aspects of the dataset.\n",
    "    \n",
    "    Displays date range and trip duration statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing trip data with pickup_datetime and trip_time columns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Prints temporal analysis results\n",
    "        \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> analyze_temporal_data(df)\n",
    "    Date range: 2024-01-01 to 2024-11-30 (334 days)\n",
    "    Trip duration: mean=20.1 min, median=16.3 min\n",
    "    \"\"\"\n",
    "    if 'pickup_datetime' in df.columns:\n",
    "        span_days = (df['pickup_datetime'].max() - df['pickup_datetime'].min()).days\n",
    "        print(f\"Date range: {df['pickup_datetime'].min().date()} to {df['pickup_datetime'].max().date()} ({span_days} days)\")\n",
    "    if 'trip_time' in df.columns:\n",
    "        print(f\"Trip duration: mean={df['trip_time'].mean()/60:.6f} min, median={df['trip_time'].median()/60:.6f} min\")\n",
    "\n",
    "\n",
    "def analyze_trip_characteristics(df):\n",
    "    \"\"\"\n",
    "    Analyze trip characteristics including distance, fare, and location information.\n",
    "    \n",
    "    Calculates and displays summary statistics for:\n",
    "    - Average trip distance\n",
    "    - Average total fare (sum of all fare components)\n",
    "    - Number of unique pickup and dropoff zones\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing trip data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Prints trip characteristics summary\n",
    "    \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> analyze_trip_characteristics(df)\n",
    "    Distance: 5.08 mi (mean) | Fare: $48.47 (avg) | Zones: 262 pickup, 263 dropoff\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    if 'trip_miles' in df.columns:\n",
    "        results.append(f\"Distance: {df['trip_miles'].mean():.6f} mi (mean)\")\n",
    "    \n",
    "    fare_cols = ['base_passenger_fare', 'tips', 'tolls', 'driver_pay']\n",
    "    fare_total = sum(df[col].mean() for col in fare_cols if col in df.columns)\n",
    "    if fare_total > 0:\n",
    "        results.append(f\"Fare: ${fare_total:.6f} (avg)\")\n",
    "    \n",
    "    if 'PULocationID' in df.columns:\n",
    "        results.append(f\"Zones: {df['PULocationID'].nunique()} pickup, {df['DOLocationID'].nunique()} dropoff\")\n",
    "    \n",
    "    print(\"\\n\" + \" | \".join(results))\n",
    "\n",
    "\n",
    "def sample_data_display(df):\n",
    "    \"\"\"\n",
    "    Display a sample of all columns from the dataset.\n",
    "    \n",
    "    Shows the first 5 rows of all columns in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame to display\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Displays a sample DataFrame using IPython.display\n",
    "    \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> sample_data_display(df)\n",
    "    Sample (11 columns):\n",
    "    [Displays first 5 rows of all columns]\n",
    "    \"\"\"\n",
    "    print(f\"\\nSample ({len(df.columns)} columns):\")\n",
    "    display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions\n",
    "\n",
    "Define visualization functions that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization Functions\n",
    "# ============================================================================\n",
    "\n",
    "def create_distribution_plots(df, output_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Create distribution plots for key trip metrics.\n",
    "    \n",
    "    Generates histograms for:\n",
    "    - Trip distance (miles)\n",
    "    - Trip duration (minutes)\n",
    "    - Base passenger fare (dollars)\n",
    "    \n",
    "    Each plot includes mean and median reference lines and uses the 99th\n",
    "    percentile to focus on the main data distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing trip data with columns: trip_miles, trip_time,\n",
    "        base_passenger_fare\n",
    "    output_dir : str, default=\"plots\"\n",
    "        Directory to save output plots\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Saves plots to output directory\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"\\nCreating distribution plots...\")\n",
    "    \n",
    "    # Trip distance distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), facecolor='white')\n",
    "    distance_data = df['trip_miles']\n",
    "    \n",
    "    # Use 99th percentile for better range (shows most data clearly)\n",
    "    upper_limit = distance_data.quantile(0.99)\n",
    "    \n",
    "    # Use ALL data (don't filter, just adjust display range)\n",
    "    # Specify bins explicitly to ensure correct binning\n",
    "    num_bins = 50\n",
    "    bin_edges = np.linspace(0, upper_limit, num_bins + 1)\n",
    "    ax.hist(distance_data, bins=bin_edges, edgecolor='black', alpha=0.7, \n",
    "            color='steelblue', linewidth=0.5)\n",
    "    ax.set_xlabel('Trip Distance (miles)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Distribution of Trip Distances', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.axvline(distance_data.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {distance_data.mean():.2f} miles')\n",
    "    ax.axvline(distance_data.median(), color='green', linestyle='--', linewidth=2,\n",
    "                label=f'Median: {distance_data.median():.2f} miles')\n",
    "    ax.set_xlim(left=0, right=upper_limit)\n",
    "    ax.legend(loc='upper right', framealpha=0.9)\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/trip_distance_distribution.png', dpi=150, \n",
    "                bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.close()\n",
    "    print(\"  [OK] Saved: trip_distance_distribution.png\")\n",
    "    \n",
    "    # Trip duration distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), facecolor='white')\n",
    "    duration_data = df['trip_time'] / 60  # Convert to minutes\n",
    "    \n",
    "    # Use 99th percentile for better range\n",
    "    upper_limit = duration_data.quantile(0.99)\n",
    "    \n",
    "    # Use ALL data (don't filter, just adjust display range)\n",
    "    # Specify bins explicitly to ensure correct binning\n",
    "    num_bins = 50\n",
    "    bin_edges = np.linspace(0, upper_limit, num_bins + 1)\n",
    "    ax.hist(duration_data, bins=bin_edges, edgecolor='black', alpha=0.7,\n",
    "            color='steelblue', linewidth=0.5)\n",
    "    ax.set_xlabel('Trip Duration (minutes)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Distribution of Trip Durations', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.axvline(duration_data.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {duration_data.mean():.2f} min')\n",
    "    ax.axvline(duration_data.median(), color='green', linestyle='--', linewidth=2,\n",
    "                label=f'Median: {duration_data.median():.2f} min')\n",
    "    ax.set_xlim(left=0, right=upper_limit)\n",
    "    ax.legend(loc='upper right', framealpha=0.9)\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/trip_duration_distribution.png', dpi=150, \n",
    "                bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.close()\n",
    "    print(\"  [OK] Saved: trip_duration_distribution.png\")\n",
    "    \n",
    "    # Fare distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), facecolor='white')\n",
    "    fare_data = df['base_passenger_fare']\n",
    "    \n",
    "    # Use 99th percentile for better range\n",
    "    upper_limit = fare_data.quantile(0.99)\n",
    "    \n",
    "    # Use ALL data (don't filter, just adjust display range)\n",
    "    # Specify bins explicitly to ensure correct binning\n",
    "    num_bins = 50\n",
    "    bin_edges = np.linspace(0, upper_limit, num_bins + 1)\n",
    "    ax.hist(fare_data, bins=bin_edges, edgecolor='black', alpha=0.7,\n",
    "            color='steelblue', linewidth=0.5)\n",
    "    ax.set_xlabel('Base Passenger Fare ($)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Distribution of Base Passenger Fares', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.axvline(fare_data.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: ${fare_data.mean():.2f}')\n",
    "    ax.axvline(fare_data.median(), color='green', linestyle='--', linewidth=2,\n",
    "                label=f'Median: ${fare_data.median():.2f}')\n",
    "    ax.set_xlim(left=0, right=upper_limit)\n",
    "    ax.legend(loc='upper right', framealpha=0.9)\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/fare_distribution.png', dpi=150, \n",
    "                bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.close()\n",
    "    print(\"  [OK] Saved: fare_distribution.png\")\n",
    "\n",
    "def create_temporal_analysis(df, output_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Create temporal analysis visualizations.\n",
    "    \n",
    "    Generates plots showing:\n",
    "    - Hourly trip distribution (24-hour pattern)\n",
    "    - Weekly trip distribution (day of week pattern)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing trip data with pickup_datetime column\n",
    "    output_dir : str, default=\"plots\"\n",
    "        Directory to save output plots\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Saves plots to output directory\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating temporal analysis...\")\n",
    "    \n",
    "    # Extract hour from pickup datetime\n",
    "    df['pickup_hour'] = df['pickup_datetime'].dt.hour\n",
    "    hourly_counts = df['pickup_hour'].value_counts().sort_index()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6), facecolor='white')\n",
    "    bars = ax.bar(hourly_counts.index, hourly_counts.values, alpha=0.7, \n",
    "                  edgecolor='black', color='steelblue', linewidth=0.5, label='All Trips')\n",
    "    ax.set_xlabel('Hour of Day', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Trips', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Trip Distribution by Hour of Day', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.set_xticks(range(24))\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax.legend(loc='upper right', framealpha=0.9, fontsize=10)\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/trips_by_hour.png', dpi=150, \n",
    "                bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.close()\n",
    "    print(\"  [OK] Saved: trips_by_hour.png\")\n",
    "    \n",
    "    # Extract day of week\n",
    "    df['day_of_week'] = df['pickup_datetime'].dt.day_name()\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    day_counts = df['day_of_week'].value_counts().reindex(day_order)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6), facecolor='white')\n",
    "    bars = ax.bar(day_counts.index, day_counts.values, alpha=0.7, \n",
    "                  edgecolor='black', color='steelblue', linewidth=0.5, label='All Trips')\n",
    "    ax.set_xlabel('Day of Week', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Trips', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Trip Distribution by Day of Week', fontsize=14, fontweight='bold', pad=15)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax.legend(loc='upper right', framealpha=0.9, fontsize=10)\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/trips_by_day.png', dpi=150, \n",
    "                bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.close()\n",
    "    print(\"  [OK] Saved: trips_by_day.png\")\n",
    "\n",
    "def create_location_analysis(df, output_dir=\"plots\", zone_lookup_dict=None):\n",
    "    \"\"\"\n",
    "    Create location-based analysis visualizations.\n",
    "    \n",
    "    Generates horizontal bar charts for:\n",
    "    - Top 15 pickup locations by frequency\n",
    "    - Top 15 dropoff locations by frequency\n",
    "    \n",
    "    If zone_lookup_dict is provided, displays zone names instead of IDs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing trip data with PULocationID and DOLocationID columns\n",
    "    output_dir : str, default=\"plots\"\n",
    "        Directory to save output plots\n",
    "    zone_lookup_dict : dict, optional\n",
    "        Dictionary mapping LocationID to zone names. If None, uses LocationID values.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Saves plots to output directory\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating location analysis...\")\n",
    "    \n",
    "    # Top pickup locations\n",
    "    top_pickups = df['PULocationID'].value_counts().head(15)\n",
    "    \n",
    "    # Use zone names if lookup dictionary is provided\n",
    "    if zone_lookup_dict:\n",
    "        pickup_labels = [zone_lookup_dict.get(zone_id, f\"Zone {zone_id}\") \n",
    "                        for zone_id in top_pickups.index]\n",
    "    else:\n",
    "        pickup_labels = [f\"Zone {zone_id}\" for zone_id in top_pickups.index]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8), facecolor='white')\n",
    "    bars = ax.barh(range(len(top_pickups)), top_pickups.values, alpha=0.7, \n",
    "                   edgecolor='black', color='steelblue', linewidth=0.5, label='Top 15 Pickup Zones')\n",
    "    ax.set_yticks(range(len(top_pickups)))\n",
    "    ax.set_yticklabels(pickup_labels, fontsize=10)\n",
    "    ax.set_xlabel('Number of Pickups', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Top 15 Pickup Locations (by frequency)', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, count) in enumerate(zip(bars, top_pickups.values)):\n",
    "        ax.text(bar.get_width() + bar.get_width()*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{count:.2f}', va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax.legend(loc='lower right', framealpha=0.9, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/top_pickup_locations.png', dpi=150, \n",
    "                bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.close()\n",
    "    print(\"  [OK] Saved: top_pickup_locations.png\")\n",
    "    \n",
    "    # Top dropoff locations\n",
    "    top_dropoffs = df['DOLocationID'].value_counts().head(15)\n",
    "    \n",
    "    # Use zone names if lookup dictionary is provided\n",
    "    if zone_lookup_dict:\n",
    "        dropoff_labels = [zone_lookup_dict.get(zone_id, f\"Zone {zone_id}\") \n",
    "                         for zone_id in top_dropoffs.index]\n",
    "    else:\n",
    "        dropoff_labels = [f\"Zone {zone_id}\" for zone_id in top_dropoffs.index]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8), facecolor='white')\n",
    "    bars = ax.barh(range(len(top_dropoffs)), top_dropoffs.values, alpha=0.7, \n",
    "                   edgecolor='black', color='steelblue', linewidth=0.5, label='Top 15 Dropoff Zones')\n",
    "    ax.set_yticks(range(len(top_dropoffs)))\n",
    "    ax.set_yticklabels(dropoff_labels, fontsize=10)\n",
    "    ax.set_xlabel('Number of Dropoffs', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Top 15 Dropoff Locations (by frequency)', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, count) in enumerate(zip(bars, top_dropoffs.values)):\n",
    "        ax.text(bar.get_width() + bar.get_width()*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{count:.2f}', va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax.legend(loc='lower right', framealpha=0.9, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/top_dropoff_locations.png', dpi=150, \n",
    "                bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.close()\n",
    "    print(\"  [OK] Saved: top_dropoff_locations.png\")\n",
    "\n",
    "\n",
    "\n",
    "def create_demand_analysis(df, output_dir=\"plots\", top_zones=20, zone_lookup_dict=None, sample_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Create comprehensive demand analysis visualizations.\n",
    "    \n",
    "    Generates multiple visualizations:\n",
    "    - Demand heatmap (hour × zone)\n",
    "    - Peak hour ranking chart\n",
    "    - Temporal pattern plot (weekday vs weekend)\n",
    "    - Demand volatility table with statistics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing trip data with pickup_datetime and PULocationID columns\n",
    "    output_dir : str, default=\"plots\"\n",
    "        Directory to save output plots\n",
    "    top_zones : int, default=20\n",
    "        Number of top zones to analyze\n",
    "    zone_lookup_dict : dict, optional\n",
    "        Dictionary mapping LocationID to zone names. If None, uses \"Zone {ID}\" format.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Saves plots and CSV to output directory\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating demand analysis outputs...\")\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    required_columns = ['pickup_datetime', 'PULocationID']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"  [WARN] Missing required columns for demand analysis: {missing_columns}\")\n",
    "        return\n",
    "\n",
    "    demand_df = (\n",
    "        df[required_columns]\n",
    "        .dropna(subset=required_columns)\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    if demand_df.empty:\n",
    "        print(\"  [WARN] No valid pickup records available for demand analysis.\")\n",
    "        return\n",
    "\n",
    "    demand_df['pickup_datetime'] = pd.to_datetime(demand_df['pickup_datetime'])\n",
    "    demand_df['pickup_hour'] = demand_df['pickup_datetime'].dt.hour\n",
    "    demand_df['pickup_date'] = demand_df['pickup_datetime'].dt.date\n",
    "    demand_df['pickup_weekday'] = demand_df['pickup_datetime'].dt.dayofweek\n",
    "    demand_df['is_weekend'] = demand_df['pickup_weekday'] >= 5\n",
    "    demand_df['PULocationID'] = demand_df['PULocationID'].astype(int)\n",
    "    # Use zone names if lookup dictionary is provided\n",
    "    if zone_lookup_dict:\n",
    "        demand_df['zone_label'] = demand_df['PULocationID'].map(\n",
    "            lambda x: zone_lookup_dict.get(x, f\"Zone {x}\")\n",
    "        )\n",
    "    else:\n",
    "        demand_df['zone_label'] = demand_df['PULocationID'].map(lambda x: f\"Zone {x}\")\n",
    "\n",
    "    zone_counts = demand_df['zone_label'].value_counts()\n",
    "    if zone_counts.empty:\n",
    "        print(\"  [WARN] Unable to compute zone frequency counts.\")\n",
    "        return\n",
    "\n",
    "    zone_order = zone_counts.head(top_zones).index.tolist()\n",
    "    filtered_df = demand_df[demand_df['zone_label'].isin(zone_order)]\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 1. Demand Heatmap (Hour × Zone)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    heatmap_counts = (\n",
    "        filtered_df\n",
    "        .groupby(['zone_label', 'pickup_hour'])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(index=zone_order, columns=range(24), fill_value=0)\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, max(6, 0.4 * len(zone_order))), facecolor='white')\n",
    "    im = ax.imshow(heatmap_counts.values, aspect='auto', cmap='YlOrRd')\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Trip Count', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Hour of Day', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Pickup Zone', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Demand Heatmap (Top {len(zone_order)} Zones)', \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.set_xticks(range(24), range(24))\n",
    "    ax.set_yticks(range(len(zone_order)), zone_order, fontsize=9)\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    # Add annotations for readability when the matrix is reasonably sized\n",
    "    if len(zone_order) <= 25:\n",
    "        max_val = heatmap_counts.values.max()\n",
    "        threshold = max_val * 0.5 if max_val > 0 else 0\n",
    "        for i, zone in enumerate(zone_order):\n",
    "            for j in range(24):\n",
    "                value = int(heatmap_counts.iloc[i, j])\n",
    "                if value == 0:\n",
    "                    continue\n",
    "                text_color = 'white' if value >= threshold else 'black'\n",
    "                ax.text(j, i, f'{int(value)}', ha='center', va='center',\n",
    "                         color=text_color, fontsize=7, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/demand_heatmap.png', dpi=150, \n",
    "                bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.close()\n",
    "    print(\"  [OK] Saved: demand_heatmap.png\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 2. Peak Hour Chart\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    zone_hourly = (\n",
    "        heatmap_counts\n",
    "        .stack()\n",
    "        .reset_index(name='trip_count')\n",
    "        .rename(columns={'level_1': 'pickup_hour'})\n",
    "    )\n",
    "\n",
    "    peak_hours = (\n",
    "        zone_hourly\n",
    "        .sort_values('trip_count', ascending=False)\n",
    "        .groupby('zone_label')\n",
    "        .head(1)\n",
    "        .sort_values('trip_count', ascending=False)\n",
    "    )\n",
    "\n",
    "    top_peak_hours = peak_hours.head(min(15, len(peak_hours)))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, max(6, 0.4 * len(top_peak_hours))), facecolor='white')\n",
    "    bars = ax.barh(\n",
    "        top_peak_hours['zone_label'],\n",
    "        top_peak_hours['trip_count'],\n",
    "        color='steelblue',\n",
    "        edgecolor='black',\n",
    "        alpha=0.85,\n",
    "        linewidth=0.5,\n",
    "        label='Peak Hour Volume'\n",
    "    )\n",
    "    ax.set_xlabel('Trips (Peak Hour Volume)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Peak Hour Ranking by Pickup Zone', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    ax.legend(loc='lower right', framealpha=0.9, fontsize=10)\n",
    "\n",
    "    for bar, hour in zip(bars, top_peak_hours['pickup_hour']):\n",
    "        width = bar.get_width()\n",
    "        ax.text(\n",
    "            width + (width * 0.01 if width > 0 else 0.5),\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            f'{int(hour):02d}:00',\n",
    "            va='center',\n",
    "            fontsize=10,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/peak_hour_chart.png', dpi=150, \n",
    "                bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.close()\n",
    "    print(\"  [OK] Saved: peak_hour_chart.png\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 3. Temporal Pattern Plot (Weekday vs Weekend)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    daily_hour_counts = (\n",
    "        demand_df\n",
    "        .groupby(['pickup_date', 'pickup_hour', 'is_weekend'])\n",
    "        .size()\n",
    "        .reset_index(name='trip_count')\n",
    "    )\n",
    "\n",
    "    hourly_profile = (\n",
    "        daily_hour_counts\n",
    "        .groupby(['is_weekend', 'pickup_hour'])['trip_count']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6), facecolor='white')\n",
    "    for is_weekend, label, color in [\n",
    "        (False, 'Weekday', 'tab:blue'),\n",
    "        (True, 'Weekend', 'tab:orange')\n",
    "    ]:\n",
    "        subset = hourly_profile[hourly_profile['is_weekend'] == is_weekend]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "        ax.plot(\n",
    "            subset['pickup_hour'].to_numpy(),\n",
    "            subset['trip_count'].to_numpy(),\n",
    "            marker='o',\n",
    "            linewidth=2.5,\n",
    "            markersize=6,\n",
    "            label=label,\n",
    "            color=color\n",
    "        )\n",
    "\n",
    "    ax.set_xticks(range(24), range(24))\n",
    "    ax.set_xlabel('Hour of Day', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Average Trips per Hour', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Weekday vs Weekend Hourly Demand Profile', \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.grid(alpha=0.3, linestyle='--')\n",
    "    ax.legend(loc='best', framealpha=0.9, fontsize=11)\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/temporal_pattern_plot.png', dpi=150, \n",
    "                bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.close()\n",
    "    print(\"  [OK] Saved: temporal_pattern_plot.png\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 4. Demand Volatility Table\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # Calculate number of trips per zone per day\n",
    "    daily_zone_counts = (\n",
    "        demand_df\n",
    "        .groupby(['zone_label', 'pickup_date'])\n",
    "        .size()\n",
    "        .reset_index(name='trip_count')\n",
    "    )\n",
    "\n",
    "    # Calculate stability metrics for each zone\n",
    "    volatility = (\n",
    "        daily_zone_counts\n",
    "        .groupby('zone_label')['trip_count']\n",
    "        .agg(['mean', 'median', 'std', 'min', 'max', 'count'])\n",
    "        .reset_index()\n",
    "        .rename(columns={\n",
    "            'mean': 'mean_daily_trips',\n",
    "            'median': 'median_daily_trips',\n",
    "            'std': 'std_daily_trips',\n",
    "            'min': 'min_daily_trips',\n",
    "            'max': 'max_daily_trips',\n",
    "            'count': 'days_observed'\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # Calculate coefficient of variation\n",
    "    volatility['std_daily_trips'] = volatility['std_daily_trips'].fillna(0)\n",
    "    volatility['coef_var'] = np.where(\n",
    "        volatility['mean_daily_trips'] > 0,\n",
    "        volatility['std_daily_trips'] / volatility['mean_daily_trips'],\n",
    "        np.nan\n",
    "    )\n",
    "    volatility['coef_var_pct'] = volatility['coef_var'] * 100\n",
    "\n",
    "    # Sort by coefficient of variation (higher CV means less stable demand)\n",
    "    volatility = volatility.sort_values('coef_var', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Save as CSV\n",
    "    volatility_path = Path(output_dir) / 'demand_volatility_table.csv'\n",
    "    volatility.to_csv(volatility_path, index=False)\n",
    "\n",
    "    # Create visualization table (show top 5)\n",
    "    n_display = min(5, len(volatility))\n",
    "    table_preview = (\n",
    "        volatility\n",
    "        .head(n_display)\n",
    "        .loc[:, ['zone_label', 'mean_daily_trips', 'median_daily_trips', 'std_daily_trips',\n",
    "                 'coef_var_pct', 'min_daily_trips', 'max_daily_trips', 'days_observed']]\n",
    "    )\n",
    "\n",
    "    # Rename columns to match the image format\n",
    "    table_display = table_preview.rename(columns={\n",
    "        'zone_label': 'Zone',\n",
    "        'mean_daily_trips': 'Mean Daily Trips',\n",
    "        'median_daily_trips': 'Median Daily Trips',\n",
    "        'std_daily_trips': 'Std Dev',\n",
    "        'coef_var_pct': 'CV (%)',\n",
    "        'min_daily_trips': 'Min Daily',\n",
    "        'max_daily_trips': 'Max Daily',\n",
    "        'days_observed': 'Days Observed'\n",
    "    })\n",
    "\n",
    "    # Format numeric columns to match image format\n",
    "    numeric_cols = table_display.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Format values for display - match image format exactly\n",
    "    for col in numeric_cols:\n",
    "        if col == 'CV (%)':\n",
    "            # Two decimal places for CV\n",
    "            table_display[col] = table_display[col].apply(\n",
    "                lambda x: f\"{x:.2f}\" if pd.notna(x) else \"\"\n",
    "            )\n",
    "        elif col == 'Mean Daily Trips':\n",
    "            # Two decimal places for Mean\n",
    "            table_display[col] = table_display[col].apply(\n",
    "                lambda x: f\"{x:.2f}\" if pd.notna(x) else \"\"\n",
    "            )\n",
    "        elif col == 'Median Daily Trips':\n",
    "            # One decimal place for Median\n",
    "            table_display[col] = table_display[col].apply(\n",
    "                lambda x: f\"{x:.2f}\" if pd.notna(x) else \"\"\n",
    "            )\n",
    "        elif col == 'Std Dev':\n",
    "            # Two decimal places for Std Dev\n",
    "            table_display[col] = table_display[col].apply(\n",
    "                lambda x: f\"{x:.2f}\" if pd.notna(x) else \"\"\n",
    "            )\n",
    "        else:\n",
    "            # Integers for Min, Max, Days\n",
    "            table_display[col] = table_display[col].apply(\n",
    "                lambda x: f\"{int(round(x))}\" if pd.notna(x) else \"\"\n",
    "            )\n",
    "    \n",
    "    # Convert to list for table display (all values are already formatted as strings)\n",
    "    cell_data = []\n",
    "    for idx, row in table_display.iterrows():\n",
    "        cell_data.append([val for val in row.values])\n",
    "\n",
    "    # Create table plot matching the image style\n",
    "    fig, ax = plt.subplots(figsize=(20, max(8, 0.7 * len(table_display) + 2)), facecolor='white')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Define column widths for 8 columns\n",
    "    col_widths = [0.25, 0.11, 0.11, 0.10, 0.10, 0.08, 0.10, 0.15]\n",
    "    \n",
    "    table = ax.table(\n",
    "        cellText=cell_data,\n",
    "        colLabels=table_display.columns.tolist(),\n",
    "        loc='center',\n",
    "        cellLoc='left',  # Left align all text like in the image\n",
    "        colWidths=col_widths\n",
    "    )\n",
    "    \n",
    "    # Table styling - uniform cell sizes\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(11)\n",
    "    table.scale(1, 3.0)\n",
    "    \n",
    "    # Green header styling\n",
    "    header_color = '#2e7d32'  # Green color\n",
    "    \n",
    "    for i, col_name in enumerate(table_display.columns):\n",
    "        cell = table[(0, i)]\n",
    "        cell.set_facecolor(header_color)\n",
    "        cell.set_text_props(weight='bold', color='white', size=12)\n",
    "        cell.set_edgecolor('white')\n",
    "        cell.set_linewidth(2.0)\n",
    "        cell.set_height(0.10)\n",
    "        cell.PAD = 0.02\n",
    "        text = cell.get_text()\n",
    "        text.set_ha('left')  # Left align header text\n",
    "    \n",
    "    # Data rows with white and light gray alternating\n",
    "    row_colors = ['#ffffff', '#f5f5f5']  # White and light gray\n",
    "    \n",
    "    for i in range(1, len(table_display) + 1):\n",
    "        row_color = row_colors[i % 2]\n",
    "        for j, col_name in enumerate(table_display.columns):\n",
    "            cell = table[(i, j)]\n",
    "            cell.set_facecolor(row_color)\n",
    "            cell.set_text_props(size=11, color='black')\n",
    "            cell.set_edgecolor('black')\n",
    "            cell.set_linewidth(0.5)  # Thin black borders\n",
    "            cell.set_height(0.09)\n",
    "            cell.PAD = 0.02\n",
    "            \n",
    "            # Left align all text\n",
    "            text = cell.get_text()\n",
    "            text.set_ha('left')\n",
    "    \n",
    "    # Set all cell borders to thin black\n",
    "    for key, cell in table.get_celld().items():\n",
    "        if key[0] == 0:\n",
    "            # Header borders remain white\n",
    "            continue\n",
    "        cell.set_edgecolor('black')\n",
    "        cell.set_linewidth(0.5)\n",
    "    \n",
    "    # Title styling to match image\n",
    "    title_text = 'Demand Volatility Table'\n",
    "    subtitle_text = 'Higher CV = More Volatile Demand.'\n",
    "    \n",
    "    ax.set_title(title_text, \n",
    "              pad=30, fontsize=20, fontweight='bold', color='black')\n",
    "    \n",
    "    # Subtitle\n",
    "    ax.text(0.5, 0.96, subtitle_text,\n",
    "           transform=ax.transAxes, ha='center', va='top',\n",
    "           fontsize=14, style='normal', color='black', weight='normal')\n",
    "    \n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax.set_facecolor('white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/demand_volatility_table.png', dpi=250, \n",
    "                bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.close()\n",
    "    print(f\"  [OK] Saved: demand_volatility_table.png\")\n",
    "    print(f\"  [OK] Saved: demand_volatility_table.csv -> {volatility_path}\")\n",
    "\n",
    "def create_manhattan_interactive_map(df, output_dir=\"plots\",\n",
    "                                     shapefile_path=\"data/taxi_zones/taxi_zones.shp\",\n",
    "                                     hours=None):\n",
    "    print(\"\\nCreating interactive NYC demand map...\")\n",
    "\n",
    "    try:\n",
    "        import geopandas as gpd\n",
    "        import plotly.express as px\n",
    "    except ImportError as exc:\n",
    "        print(f\"  [WARN] Missing dependency for interactive map: {exc}. Skipping HTML map.\")\n",
    "        return\n",
    "\n",
    "    shapefile_path = Path(shapefile_path)\n",
    "    if not shapefile_path.exists():\n",
    "        print(f\"  [WARN] Taxi zone shapefile not found at {shapefile_path}. Skipping HTML map.\")\n",
    "        return\n",
    "\n",
    "    required_columns = ['pickup_datetime', 'PULocationID']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"  [WARN] Missing columns for interactive map: {missing_columns}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        taxi_zones = gpd.read_file(shapefile_path)\n",
    "    except Exception as exc:\n",
    "        print(f\"  [WARN] Failed to read shapefile: {exc}\")\n",
    "        return\n",
    "\n",
    "    if 'LocationID' not in taxi_zones.columns:\n",
    "        print(\"  [WARN] Shapefile missing 'LocationID'. Skipping HTML map.\")\n",
    "        return\n",
    "\n",
    "    # Use all zones in NYC (not just Manhattan)\n",
    "    all_zones = taxi_zones[['LocationID', 'zone', 'geometry']].copy()\n",
    "    if 'borough' in taxi_zones.columns:\n",
    "        all_zones['borough'] = taxi_zones['borough']\n",
    "    all_zones.rename(columns={'zone': 'Zone'}, inplace=True)\n",
    "    all_zones['LocationID'] = all_zones['LocationID'].astype(int)\n",
    "    all_zones = all_zones.to_crs(epsg=4326)\n",
    "    \n",
    "    if all_zones.empty:\n",
    "        print(\"  [WARN] No zones found. Skipping HTML map.\")\n",
    "        return\n",
    "\n",
    "    demand_df = df[required_columns].dropna(subset=required_columns).copy()\n",
    "    demand_df['pickup_datetime'] = pd.to_datetime(demand_df['pickup_datetime'])\n",
    "    demand_df['pickup_hour'] = demand_df['pickup_datetime'].dt.hour\n",
    "    demand_df['PULocationID'] = demand_df['PULocationID'].astype(int)\n",
    "\n",
    "    if hours is not None:\n",
    "        hours = sorted(set([h for h in hours if 0 <= h <= 23]))\n",
    "        if not hours:\n",
    "            print(\"  [WARN] Provided hours list is empty after validation.\")\n",
    "            return\n",
    "        demand_df = demand_df[demand_df['pickup_hour'].isin(hours)]\n",
    "\n",
    "    hourly_counts = (\n",
    "        demand_df\n",
    "        .groupby(['PULocationID', 'pickup_hour'])\n",
    "        .size()\n",
    "        .reset_index(name='trip_count')\n",
    "    )\n",
    "\n",
    "    if hourly_counts.empty:\n",
    "        print(\"  [WARN] No trip data available for interactive map.\")\n",
    "        return\n",
    "\n",
    "    merged = hourly_counts.merge(\n",
    "        all_zones[['LocationID', 'Zone']],\n",
    "        left_on='PULocationID',\n",
    "        right_on='LocationID',\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    if merged.empty:\n",
    "        print(\"  [WARN] No overlapping records for interactive map.\")\n",
    "        return\n",
    "\n",
    "    merged = (\n",
    "        merged\n",
    "        .groupby(['LocationID', 'pickup_hour', 'Zone'], as_index=False)['trip_count']\n",
    "        .sum()\n",
    "    )\n",
    "\n",
    "    zone_lookup = merged[['LocationID', 'Zone']].drop_duplicates()\n",
    "    full_index = pd.MultiIndex.from_product(\n",
    "        [zone_lookup['LocationID'].unique(), list(range(24))],\n",
    "        names=['LocationID', 'pickup_hour']\n",
    "    )\n",
    "    merged = (\n",
    "        merged\n",
    "        .set_index(['LocationID', 'pickup_hour'])\n",
    "        .reindex(full_index, fill_value=0)\n",
    "        .reset_index()\n",
    "    )\n",
    "    merged = merged.drop(columns=['Zone'], errors='ignore')\n",
    "    merged = merged.merge(zone_lookup, on='LocationID', how='left')\n",
    "\n",
    "    merged['pickup_hour'] = merged['pickup_hour'].astype(int)\n",
    "    merged['LocationID'] = merged['LocationID'].astype(int).astype(str)\n",
    "    geojson = json.loads(all_zones.to_json())\n",
    "\n",
    "    vmax = merged['trip_count'].max()\n",
    "    if vmax == 0:\n",
    "        print(\"  [WARN] Trip counts are zero across all hours. Skipping HTML map.\")\n",
    "        return\n",
    "\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    fig = px.choropleth(\n",
    "        merged,\n",
    "        geojson=geojson,\n",
    "        locations='LocationID',\n",
    "        color='trip_count',\n",
    "        animation_frame='pickup_hour',\n",
    "        hover_name='Zone',\n",
    "        hover_data={'trip_count': ':,', 'pickup_hour': True, 'LocationID': False},\n",
    "        featureidkey='properties.LocationID',\n",
    "        color_continuous_scale='YlOrRd',\n",
    "        range_color=(0, vmax),\n",
    "        title='NYC Hourly Pickup Demand (Interactive)',\n",
    "        category_orders={'pickup_hour': list(range(24))}\n",
    "    )\n",
    "\n",
    "    fig.update_geos(fitbounds='locations', visible=False)\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, t=50, b=0),\n",
    "        coloraxis_colorbar=dict(title='Trips per Hour'),\n",
    "        updatemenus=[{\n",
    "            \"buttons\": [\n",
    "                {\"args\": [None, {\"frame\": {\"duration\": 500, \"redraw\": True},\n",
    "                                 \"fromcurrent\": True,\n",
    "                                 \"transition\": {\"duration\": 200}}],\n",
    "                 \"label\": \"Play\",\n",
    "                 \"method\": \"animate\"},\n",
    "                {\"args\": [[None], {\"mode\": \"immediate\",\n",
    "                                   \"frame\": {\"duration\": 0, \"redraw\": True},\n",
    "                                   \"transition\": {\"duration\": 0}}],\n",
    "                 \"label\": \"Pause\",\n",
    "                 \"method\": \"animate\"}\n",
    "            ],\n",
    "            \"direction\": \"left\",\n",
    "            \"pad\": {\"r\": 10, \"t\": 40},\n",
    "            \"showactive\": False,\n",
    "            \"type\": \"buttons\",\n",
    "            \"x\": 0.1,\n",
    "            \"y\": -0.05\n",
    "        }]\n",
    "    )\n",
    "\n",
    "    # Customize slider labels to show HH:00 format\n",
    "    if 'sliders' in fig.layout and len(fig.layout.sliders) > 0:\n",
    "        slider = fig.layout.sliders[0]\n",
    "        for step in slider['steps']:\n",
    "            hour = int(float(step['label']))\n",
    "            step['label'] = f\"{hour:02d}:00\"\n",
    "        slider['currentvalue']['prefix'] = 'Hour: '\n",
    "\n",
    "    output_path = Path(output_dir) / 'nyc_hourly_interactive_map.html'\n",
    "    fig.write_html(output_path)\n",
    "    print(f\"  [OK] Saved: nyc_hourly_interactive_map.html -> {output_path}\")\n",
    "\n",
    "def create_outlier_analysis(df, output_dir=\"plots\"):\n",
    "    print(\"\\nCreating outlier analysis...\")\n",
    "    \n",
    "    # Directly specify numeric columns to avoid memory-intensive select_dtypes\n",
    "    # This avoids copying the entire DataFrame\n",
    "    potential_numeric_cols = ['trip_miles', 'trip_time', 'base_passenger_fare', \n",
    "                              'tips', 'tolls', 'airport_fee', 'driver_pay']\n",
    "    \n",
    "    # Check which columns exist and are numeric (without copying DataFrame)\n",
    "    numeric_cols = []\n",
    "    for col in potential_numeric_cols:\n",
    "        if col in df.columns:\n",
    "            # Check dtype without copying\n",
    "            if pd.api.types.is_numeric_dtype(df[col].dtype):\n",
    "                numeric_cols.append(col)\n",
    "    \n",
    "    if not numeric_cols:\n",
    "        print(\"  No numeric columns for outlier analysis\")\n",
    "        return\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Box Plots for Key Fields (showing outliers)\n",
    "    # Select 6 key fields for box plots\n",
    "    key_fields = ['trip_miles', 'trip_time', 'base_passenger_fare', 'tips', 'tolls', 'airport_fee']\n",
    "    # Filter to only fields that exist\n",
    "    key_fields = [f for f in key_fields if f in numeric_cols]\n",
    "    \n",
    "    if len(key_fields) >= 6:\n",
    "        key_fields = key_fields[:6]  # Take first 6\n",
    "    elif len(key_fields) > 0:\n",
    "        # If less than 6, use what we have\n",
    "        pass\n",
    "    else:\n",
    "        # Fallback to other numeric fields if key fields not available\n",
    "        key_fields = numeric_cols[:6] if len(numeric_cols) >= 6 else numeric_cols\n",
    "    \n",
    "    # For very large datasets, sample data for boxplot visualization to save memory\n",
    "    # We'll still calculate statistics on full dataset, but plot on sample\n",
    "    sample_size = min(100000, total_rows)  # Sample up to 100k rows for plotting\n",
    "    use_sample = total_rows > 500000  # Only sample if dataset is very large\n",
    "    \n",
    "    if key_fields:\n",
    "        # Create 2x3 grid for 6 subplots\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12), facecolor='white')\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, col in enumerate(key_fields):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Calculate outlier stats for this specific field\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            median = df[col].median()\n",
    "            \n",
    "            if IQR > 0:\n",
    "                # Standard IQR method (3 * IQR from median)\n",
    "                lower_bound = Q1 - 3 * IQR\n",
    "                upper_bound = Q3 + 3 * IQR\n",
    "                # Use mask for more memory-efficient filtering\n",
    "                outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "                outlier_count = outlier_mask.sum()\n",
    "                outlier_pct = outlier_count / total_rows * 100\n",
    "            elif IQR == 0 and median == 0:\n",
    "                # Special case: sparse data with most values = 0 (like tips, tolls, airport_fee)\n",
    "                # Use 95th percentile method: values above 95th percentile are outliers\n",
    "                p95 = df[col].quantile(0.95)\n",
    "                outlier_mask = df[col] > p95\n",
    "                outlier_count = outlier_mask.sum()\n",
    "                outlier_pct = outlier_count / total_rows * 100\n",
    "            elif IQR == 0:\n",
    "                # IQR is 0 but median is not 0 - all values are the same\n",
    "                outlier_count = 0\n",
    "                outlier_pct = 0.0\n",
    "            else:\n",
    "                outlier_count = 0\n",
    "                outlier_pct = 0.0\n",
    "            \n",
    "            # Create box plot - use sample for large datasets to save memory\n",
    "            if use_sample:\n",
    "                box_data = df[col].dropna().sample(n=sample_size, random_state=42)\n",
    "            else:\n",
    "                box_data = df[col].dropna()\n",
    "            bp = ax.boxplot([box_data], vert=True, patch_artist=True, \n",
    "                           showfliers=True, flierprops=dict(marker='o', markersize=4, \n",
    "                           markerfacecolor='red', markeredgecolor='red', alpha=0.5))\n",
    "            \n",
    "            # Style the box plot\n",
    "            for patch in bp['boxes']:\n",
    "                patch.set_facecolor('lightblue')\n",
    "                patch.set_alpha(0.7)\n",
    "            \n",
    "            ax.set_title(f'{col}\\n(Outliers: {outlier_count:,} / {outlier_pct:.2f}%)', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('Value', fontsize=10, fontweight='bold')\n",
    "            # Add legend for boxplot explanation\n",
    "            legend_elements = [\n",
    "                Patch(facecolor='lightblue', alpha=0.7, label='Normal Range (IQR)'),\n",
    "                Patch(facecolor='red', alpha=0.5, label='Outliers')\n",
    "            ]\n",
    "            ax.legend(handles=legend_elements, loc='upper right', fontsize=9, framealpha=0.9)\n",
    "            ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "            ax.set_facecolor('white')\n",
    "        \n",
    "        # Hide extra subplots if we have less than 6\n",
    "        for idx in range(len(key_fields), 6):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        fig.patch.set_facecolor('white')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/outlier_boxplots.png', dpi=150, \n",
    "                    bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "        plt.close()\n",
    "        print(\"  [OK] Saved: outlier_boxplots.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zone Lookup Functions\n",
    "\n",
    "Define zoon lookup functions that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Zone lookup loaded: 265  zones\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Zone Lookup Loading Functions\n",
    "# ============================================================================\n",
    "def load_zone_lookup(lookup_file=\"taxi_zone_lookup.csv\"):\n",
    "    \"\"\"Load zone ID to location name mapping table.\n",
    "    \n",
    "    Reads a CSV file containing LocationID to Zone name and Borough mappings.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lookup_file : str, default=\"taxi_zone_lookup.csv\"\n",
    "        Path to zone lookup CSV file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (zone_dict, zone_with_borough_dict, zone_lookup_df)\n",
    "        - zone_dict: Dictionary mapping LocationID -> Zone name\n",
    "        - zone_with_borough_dict: Dictionary mapping LocationID -> Borough - Zone name\n",
    "        - zone_lookup_df: Complete lookup DataFrame\n",
    "        \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> zone_dict, zone_with_borough, df = load_zone_lookup()\n",
    "    [OK] Zone lookup loaded: 265 zones\n",
    "    \"\"\"\n",
    "    \n",
    "    lookup_path = Path(lookup_file)\n",
    "    if lookup_path.exists():\n",
    "        try:\n",
    "            zone_lookup = pd.read_csv(lookup_path)\n",
    "            # Check if required columns exist\n",
    "            required_cols = ['LocationID', 'Zone', 'Borough']\n",
    "            missing_cols = [col for col in required_cols if col not in zone_lookup.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"[ERROR] Zone lookup file missing required columns: {missing_cols}\")\n",
    "                return None, None, None\n",
    "            \n",
    "            zone_dict = dict(zip(zone_lookup['LocationID'], zone_lookup['Zone']))\n",
    "            zone_with_borough_dict = dict(zip(\n",
    "                zone_lookup['LocationID'], \n",
    "                zone_lookup['Borough'] + ' - ' + zone_lookup['Zone']\n",
    "            ))\n",
    "            print(f\"[OK] Zone lookup loaded: {len(zone_dict)}  zones\")\n",
    "            return zone_dict, zone_with_borough_dict, zone_lookup\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load zone lookup file: {e}\")\n",
    "            return None, None, None\n",
    "    else:\n",
    "        print(f\"[WARN] Zone lookup file not found: {lookup_file}\")\n",
    "        return None, None, None\n",
    "\n",
    "def get_zone_name(zone_id, use_borough=False):\n",
    "    \"\"\"\n",
    "    Convert zone ID to location name.\n",
    "    \n",
    "    Uses the loaded zone lookup dictionary to convert a zone ID to its\n",
    "    corresponding zone name or borough-zone name.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    zone_id : int\n",
    "        Zone ID to convert\n",
    "    use_borough : bool, default=False\n",
    "        Whether to include Borough name in the result\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Location name (e.g., \"Manhattan - Times Square\" or \"Times Square\")\n",
    "    \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> get_zone_name(230)\n",
    "    'Times Square'\n",
    "    >>> get_zone_name(230, use_borough=True)\n",
    "    'Manhattan - Times Square'\n",
    "    \"\"\"\n",
    "    # Safely access global variables\n",
    "    try:\n",
    "        zone_dict = globals().get('zone_lookup_dict')\n",
    "        zone_with_borough = globals().get('zone_lookup_with_borough')\n",
    "    except:\n",
    "        zone_dict = None\n",
    "        zone_with_borough = None\n",
    "    \n",
    "    if zone_dict is None:\n",
    "        return f\"Zone {zone_id}\"\n",
    "    if use_borough and zone_with_borough:\n",
    "        return zone_with_borough.get(zone_id, f\"Zone {zone_id}\")\n",
    "    else:\n",
    "        return zone_dict.get(zone_id, f\"Zone {zone_id}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load Zone Lookup\n",
    "# ============================================================================\n",
    "zone_lookup_dict, zone_lookup_with_borough, zone_lookup_df = load_zone_lookup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Overview\n",
    "\n",
    "## Introduction to the Dataset\n",
    "\n",
    "The FHVHV (For-Hire Vehicle High Volume) dataset contains trip records from New York City in 2024.\n",
    "\n",
    "### Dataset Information\n",
    "\n",
    "- **Dataset Name**: FHVHV (For-Hire Vehicle High Volume) Trip Data\n",
    "- **Location**: New York City\n",
    "- **Year**: 2024\n",
    "- **Number of Files**: 12 monthly files (one per month)\n",
    "- **Data Format**: Parquet\n",
    "- **Total Data Size**: Approximately 5.4 GB\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "The original dataset contains 24 columns. We perform **feature selection** during data loading to only load the columns needed for analysis:\n",
    "\n",
    "**Selected Columns (11 total):**\n",
    "- `pickup_datetime`, `dropoff_datetime`: Trip timestamps\n",
    "- `PULocationID`, `DOLocationID`: Pickup and dropoff location IDs\n",
    "- `trip_miles`: Trip distance in miles\n",
    "- `trip_time`: Trip duration in seconds\n",
    "- `base_passenger_fare`, `tips`, `tolls`, `airport_fee`, `driver_pay`: Fare components\n",
    "\n",
    "This feature selection reduces memory usage and processing time while retaining all essential information for demand analysis.\n",
    "\n",
    "### Loading the Data\n",
    "\n",
    "We load the data with 1% sampling to optimize memory usage and processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FHVHV Trip Data 2024 - Complete Analysis and Visualization\n",
      "================================================================================\n",
      "Dataset: 12 files, 5.379363 GB total\n",
      "\n",
      "Loading data from all 12 months...\n",
      "Note: Using 1% sampling to optimize memory usage and processing time\n",
      "This may take several minutes...\n",
      "Found 12 parquet files\n",
      "\n",
      "================================================================================\n",
      "Step 1: Loading Sample Data with All Columns\n",
      "================================================================================\n",
      "\n",
      "Loading first file with all columns: fhvhv_tripdata_2024-01.parquet...\n",
      "\n",
      "  Original dataset has 24 columns:\n",
      "     1. hvfhs_license_num\n",
      "     2. dispatching_base_num\n",
      "     3. originating_base_num\n",
      "     4. request_datetime\n",
      "     5. on_scene_datetime\n",
      "     6. pickup_datetime\n",
      "     7. dropoff_datetime\n",
      "     8. PULocationID\n",
      "     9. DOLocationID\n",
      "    10. trip_miles\n",
      "    11. trip_time\n",
      "    12. base_passenger_fare\n",
      "    13. tolls\n",
      "    14. bcf\n",
      "    15. sales_tax\n",
      "    16. congestion_surcharge\n",
      "    17. airport_fee\n",
      "    18. tips\n",
      "    19. driver_pay\n",
      "    20. shared_request_flag\n",
      "    21. shared_match_flag\n",
      "    22. access_a_ride_flag\n",
      "    23. wav_request_flag\n",
      "    24. wav_match_flag\n",
      "\n",
      "  First 5 rows of original data (all 24 columns):\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hvfhs_license_num</th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>originating_base_num</th>\n",
       "      <th>request_datetime</th>\n",
       "      <th>on_scene_datetime</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>base_passenger_fare</th>\n",
       "      <th>tolls</th>\n",
       "      <th>bcf</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>tips</th>\n",
       "      <th>driver_pay</th>\n",
       "      <th>shared_request_flag</th>\n",
       "      <th>shared_match_flag</th>\n",
       "      <th>access_a_ride_flag</th>\n",
       "      <th>wav_request_flag</th>\n",
       "      <th>wav_match_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2024-01-01 00:21:47</td>\n",
       "      <td>2024-01-01 00:25:06</td>\n",
       "      <td>2024-01-01 00:28:08</td>\n",
       "      <td>2024-01-01 01:05:39</td>\n",
       "      <td>161</td>\n",
       "      <td>158</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2251</td>\n",
       "      <td>45.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>4.05</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.18</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2024-01-01 00:10:56</td>\n",
       "      <td>2024-01-01 00:11:08</td>\n",
       "      <td>2024-01-01 00:12:53</td>\n",
       "      <td>2024-01-01 00:20:05</td>\n",
       "      <td>137</td>\n",
       "      <td>79</td>\n",
       "      <td>1.57</td>\n",
       "      <td>432</td>\n",
       "      <td>10.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.12</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2024-01-01 00:20:04</td>\n",
       "      <td>2024-01-01 00:21:51</td>\n",
       "      <td>2024-01-01 00:23:05</td>\n",
       "      <td>2024-01-01 00:35:16</td>\n",
       "      <td>79</td>\n",
       "      <td>186</td>\n",
       "      <td>1.98</td>\n",
       "      <td>731</td>\n",
       "      <td>18.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.47</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2024-01-01 00:35:46</td>\n",
       "      <td>2024-01-01 00:39:59</td>\n",
       "      <td>2024-01-01 00:41:04</td>\n",
       "      <td>2024-01-01 00:56:34</td>\n",
       "      <td>234</td>\n",
       "      <td>148</td>\n",
       "      <td>1.99</td>\n",
       "      <td>930</td>\n",
       "      <td>17.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.35</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2024-01-01 00:48:19</td>\n",
       "      <td>2024-01-01 00:56:23</td>\n",
       "      <td>2024-01-01 00:57:21</td>\n",
       "      <td>2024-01-01 01:10:02</td>\n",
       "      <td>148</td>\n",
       "      <td>97</td>\n",
       "      <td>2.65</td>\n",
       "      <td>761</td>\n",
       "      <td>38.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.06</td>\n",
       "      <td>3.43</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.63</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hvfhs_license_num dispatching_base_num originating_base_num  \\\n",
       "0            HV0003               B03404               B03404   \n",
       "1            HV0003               B03404               B03404   \n",
       "2            HV0003               B03404               B03404   \n",
       "3            HV0003               B03404               B03404   \n",
       "4            HV0003               B03404               B03404   \n",
       "\n",
       "     request_datetime   on_scene_datetime     pickup_datetime  \\\n",
       "0 2024-01-01 00:21:47 2024-01-01 00:25:06 2024-01-01 00:28:08   \n",
       "1 2024-01-01 00:10:56 2024-01-01 00:11:08 2024-01-01 00:12:53   \n",
       "2 2024-01-01 00:20:04 2024-01-01 00:21:51 2024-01-01 00:23:05   \n",
       "3 2024-01-01 00:35:46 2024-01-01 00:39:59 2024-01-01 00:41:04   \n",
       "4 2024-01-01 00:48:19 2024-01-01 00:56:23 2024-01-01 00:57:21   \n",
       "\n",
       "     dropoff_datetime  PULocationID  DOLocationID  trip_miles  trip_time  \\\n",
       "0 2024-01-01 01:05:39           161           158        2.83       2251   \n",
       "1 2024-01-01 00:20:05           137            79        1.57        432   \n",
       "2 2024-01-01 00:35:16            79           186        1.98        731   \n",
       "3 2024-01-01 00:56:34           234           148        1.99        930   \n",
       "4 2024-01-01 01:10:02           148            97        2.65        761   \n",
       "\n",
       "   base_passenger_fare  tolls   bcf  sales_tax  congestion_surcharge  \\\n",
       "0                45.61    0.0  1.25       4.05                  2.75   \n",
       "1                10.05    0.0  0.28       0.89                  2.75   \n",
       "2                18.07    0.0  0.50       1.60                  2.75   \n",
       "3                17.17    0.0  0.47       1.52                  2.75   \n",
       "4                38.67    0.0  1.06       3.43                  2.75   \n",
       "\n",
       "   airport_fee  tips  driver_pay shared_request_flag shared_match_flag  \\\n",
       "0          0.0   0.0       40.18                   N                 N   \n",
       "1          0.0   0.0        6.12                   N                 N   \n",
       "2          0.0   0.0        9.47                   N                 N   \n",
       "3          0.0   0.0       11.35                   N                 N   \n",
       "4          0.0   0.0       28.63                   N                 N   \n",
       "\n",
       "  access_a_ride_flag wav_request_flag wav_match_flag  \n",
       "0                  N                N              N  \n",
       "1                  N                N              N  \n",
       "2                  N                N              N  \n",
       "3                  N                N              N  \n",
       "4                  N                N              N  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Step 2: Feature Selection\n",
      "================================================================================\n",
      "\n",
      "  Selected 11 columns for analysis:\n",
      "     1. pickup_datetime\n",
      "     2. dropoff_datetime\n",
      "     3. PULocationID\n",
      "     4. DOLocationID\n",
      "     5. trip_miles\n",
      "     6. trip_time\n",
      "     7. base_passenger_fare\n",
      "     8. tips\n",
      "     9. tolls\n",
      "    10. airport_fee\n",
      "    11. driver_pay\n",
      "\n",
      "  Excluded 13 columns (not needed for this analysis):\n",
      "     1. hvfhs_license_num\n",
      "     2. dispatching_base_num\n",
      "     3. originating_base_num\n",
      "     4. request_datetime\n",
      "     5. on_scene_datetime\n",
      "     6. bcf\n",
      "     7. sales_tax\n",
      "     8. congestion_surcharge\n",
      "     9. shared_request_flag\n",
      "    10. shared_match_flag\n",
      "    11. access_a_ride_flag\n",
      "    12. wav_request_flag\n",
      "    13. wav_match_flag\n",
      "\n",
      "  First 5 rows after feature selection (11 columns):\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>base_passenger_fare</th>\n",
       "      <th>tips</th>\n",
       "      <th>tolls</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>driver_pay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:28:08</td>\n",
       "      <td>2024-01-01 01:05:39</td>\n",
       "      <td>161</td>\n",
       "      <td>158</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2251</td>\n",
       "      <td>45.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 00:12:53</td>\n",
       "      <td>2024-01-01 00:20:05</td>\n",
       "      <td>137</td>\n",
       "      <td>79</td>\n",
       "      <td>1.57</td>\n",
       "      <td>432</td>\n",
       "      <td>10.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 00:23:05</td>\n",
       "      <td>2024-01-01 00:35:16</td>\n",
       "      <td>79</td>\n",
       "      <td>186</td>\n",
       "      <td>1.98</td>\n",
       "      <td>731</td>\n",
       "      <td>18.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 00:41:04</td>\n",
       "      <td>2024-01-01 00:56:34</td>\n",
       "      <td>234</td>\n",
       "      <td>148</td>\n",
       "      <td>1.99</td>\n",
       "      <td>930</td>\n",
       "      <td>17.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 00:57:21</td>\n",
       "      <td>2024-01-01 01:10:02</td>\n",
       "      <td>148</td>\n",
       "      <td>97</td>\n",
       "      <td>2.65</td>\n",
       "      <td>761</td>\n",
       "      <td>38.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_datetime    dropoff_datetime  PULocationID  DOLocationID  \\\n",
       "0 2024-01-01 00:28:08 2024-01-01 01:05:39           161           158   \n",
       "1 2024-01-01 00:12:53 2024-01-01 00:20:05           137            79   \n",
       "2 2024-01-01 00:23:05 2024-01-01 00:35:16            79           186   \n",
       "3 2024-01-01 00:41:04 2024-01-01 00:56:34           234           148   \n",
       "4 2024-01-01 00:57:21 2024-01-01 01:10:02           148            97   \n",
       "\n",
       "   trip_miles  trip_time  base_passenger_fare  tips  tolls  airport_fee  \\\n",
       "0        2.83       2251                45.61   0.0    0.0          0.0   \n",
       "1        1.57        432                10.05   0.0    0.0          0.0   \n",
       "2        1.98        731                18.07   0.0    0.0          0.0   \n",
       "3        1.99        930                17.17   0.0    0.0          0.0   \n",
       "4        2.65        761                38.67   0.0    0.0          0.0   \n",
       "\n",
       "   driver_pay  \n",
       "0       40.18  \n",
       "1        6.12  \n",
       "2        9.47  \n",
       "3       11.35  \n",
       "4       28.63  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Step 3: Loading 1.000000% of data from all 12 files...\n",
      "================================================================================\n",
      "\n",
      "  Loading file 1/12: fhvhv_tripdata_2024-01.parquet...\n",
      "    [OK] Loaded 196,639 rows (1.000000% of 19,663,930)\n",
      "\n",
      "  Loading file 2/12: fhvhv_tripdata_2024-02.parquet...\n",
      "    [OK] Loaded 193,591 rows (1.000000% of 19,359,148)\n",
      "\n",
      "  Loading file 3/12: fhvhv_tripdata_2024-03.parquet...\n",
      "    [OK] Loaded 212,808 rows (1.000000% of 21,280,788)\n",
      "\n",
      "  Loading file 4/12: fhvhv_tripdata_2024-04.parquet...\n",
      "    [OK] Loaded 197,330 rows (1.000000% of 19,733,038)\n",
      "\n",
      "  Loading file 5/12: fhvhv_tripdata_2024-05.parquet...\n",
      "    [OK] Loaded 207,045 rows (1.000000% of 20,704,538)\n",
      "\n",
      "  Loading file 6/12: fhvhv_tripdata_2024-06.parquet...\n",
      "    [OK] Loaded 201,232 rows (1.000000% of 20,123,226)\n",
      "\n",
      "  Loading file 7/12: fhvhv_tripdata_2024-07.parquet...\n",
      "    [OK] Loaded 191,829 rows (1.000000% of 19,182,934)\n",
      "\n",
      "  Loading file 8/12: fhvhv_tripdata_2024-08.parquet...\n",
      "    [OK] Loaded 191,284 rows (1.000000% of 19,128,392)\n",
      "\n",
      "  Loading file 9/12: fhvhv_tripdata_2024-09.parquet...\n",
      "    [OK] Loaded 192,098 rows (1.000000% of 19,209,788)\n",
      "\n",
      "  Loading file 10/12: fhvhv_tripdata_2024-10.parquet...\n",
      "    [OK] Loaded 200,283 rows (1.000000% of 20,028,282)\n",
      "\n",
      "  Loading file 11/12: fhvhv_tripdata_2024-11.parquet...\n",
      "    [OK] Loaded 199,875 rows (1.000000% of 19,987,533)\n",
      "\n",
      "  Loading file 12/12: fhvhv_tripdata_2024-12.parquet...\n",
      "    [OK] Loaded 210,689 rows (1.000000% of 21,068,851)\n",
      "\n",
      "Combining 12 dataframes...\n",
      "[OK] DataFrame ready: 2,394,703 rows, 11 columns\n",
      "  Memory usage: ~182.701462 MB\n",
      "\n",
      "================================================================================\n",
      "Data loading complete!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Data Loading Summary\n",
      "================================================================================\n",
      "\n",
      "[OK] Data loaded successfully!\n",
      "  Total records: 2,394,703\n",
      "  Total columns: 11\n",
      "  Memory usage: ~182.701462 MB\n",
      "\n",
      "  Column names (11):\n",
      "    1. pickup_datetime\n",
      "    2. dropoff_datetime\n",
      "    3. PULocationID\n",
      "    4. DOLocationID\n",
      "    5. trip_miles\n",
      "    6. trip_time\n",
      "    7. base_passenger_fare\n",
      "    8. tips\n",
      "    9. tolls\n",
      "    10. airport_fee\n",
      "    11. driver_pay\n",
      "\n",
      "  Data types:\n",
      "    pickup_datetime: datetime64[us]\n",
      "    dropoff_datetime: datetime64[us]\n",
      "    PULocationID: int32\n",
      "    DOLocationID: int32\n",
      "    trip_miles: float64\n",
      "    trip_time: int64\n",
      "    base_passenger_fare: float64\n",
      "    tips: float64\n",
      "    tolls: float64\n",
      "    airport_fee: float64\n",
      "    driver_pay: float64\n",
      "\n",
      "[Preview] First 5 records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>base_passenger_fare</th>\n",
       "      <th>tips</th>\n",
       "      <th>tolls</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>driver_pay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-09 22:11:03</td>\n",
       "      <td>2024-01-09 22:29:46</td>\n",
       "      <td>230</td>\n",
       "      <td>112</td>\n",
       "      <td>5.466</td>\n",
       "      <td>1123</td>\n",
       "      <td>26.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-26 08:07:17</td>\n",
       "      <td>2024-01-26 08:35:38</td>\n",
       "      <td>85</td>\n",
       "      <td>77</td>\n",
       "      <td>4.290</td>\n",
       "      <td>1701</td>\n",
       "      <td>27.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-19 02:17:05</td>\n",
       "      <td>2024-01-19 02:29:12</td>\n",
       "      <td>220</td>\n",
       "      <td>243</td>\n",
       "      <td>2.550</td>\n",
       "      <td>727</td>\n",
       "      <td>15.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-21 01:44:00</td>\n",
       "      <td>2024-01-21 02:08:30</td>\n",
       "      <td>164</td>\n",
       "      <td>80</td>\n",
       "      <td>6.370</td>\n",
       "      <td>1470</td>\n",
       "      <td>24.57</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 14:19:17</td>\n",
       "      <td>2024-01-01 14:28:14</td>\n",
       "      <td>260</td>\n",
       "      <td>82</td>\n",
       "      <td>1.681</td>\n",
       "      <td>537</td>\n",
       "      <td>12.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_datetime    dropoff_datetime  PULocationID  DOLocationID  \\\n",
       "0 2024-01-09 22:11:03 2024-01-09 22:29:46           230           112   \n",
       "1 2024-01-26 08:07:17 2024-01-26 08:35:38            85            77   \n",
       "2 2024-01-19 02:17:05 2024-01-19 02:29:12           220           243   \n",
       "3 2024-01-21 01:44:00 2024-01-21 02:08:30           164            80   \n",
       "4 2024-01-01 14:19:17 2024-01-01 14:28:14           260            82   \n",
       "\n",
       "   trip_miles  trip_time  base_passenger_fare  tips  tolls  airport_fee  \\\n",
       "0       5.466       1123                26.48  0.00    5.8          0.0   \n",
       "1       4.290       1701                27.49  0.00    0.0          0.0   \n",
       "2       2.550        727                15.14  0.00    0.0          0.0   \n",
       "3       6.370       1470                24.57  3.01    0.0          0.0   \n",
       "4       1.681        537                12.32  0.00    0.0          0.0   \n",
       "\n",
       "   driver_pay  \n",
       "0       20.88  \n",
       "1       24.88  \n",
       "2       10.19  \n",
       "3       23.00  \n",
       "4        7.32  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Data is ready for analysis in the next sections.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Data Loading\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FHVHV Trip Data 2024 - Complete Analysis and Visualization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get file statistics (using functions defined in section 3)\n",
    "parquet_files = get_file_statistics()\n",
    "\n",
    "if not parquet_files:\n",
    "    print(\"No parquet files found!\")\n",
    "    df = None\n",
    "else:\n",
    "    # Load data (using 20% sampling to save memory and processing time)\n",
    "    print(\"\\nLoading data from all 12 months...\")\n",
    "    print(\"Note: Using 1% sampling to optimize memory usage and processing time\")\n",
    "    print(\"This may take several minutes...\")\n",
    "    \n",
    "    try:\n",
    "        parquet_files = sorted(glob(\"fhvhv_tripdata_*.parquet\"))\n",
    "        \n",
    "        if not parquet_files:\n",
    "            print(\"[ERROR] No parquet files found in current directory\")\n",
    "            df = None\n",
    "        else:\n",
    "            print(f\"Found {len(parquet_files)} parquet files\")\n",
    "            \n",
    "            # Step 1: Load first file with ALL columns to show original data structure\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"Step 1: Loading Sample Data with All Columns\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            first_file = parquet_files[0]\n",
    "            print(f\"\\nLoading first file with all columns: {Path(first_file).name}...\")\n",
    "            # Read full file first, then take first 1000 rows for display\n",
    "            df_full_temp = pd.read_parquet(first_file)\n",
    "            df_full_sample = df_full_temp.head(1000).copy()  # Take first 1000 rows to sample\n",
    "            del df_full_temp\n",
    "            gc.collect()\n",
    "            \n",
    "            all_columns = list(df_full_sample.columns)\n",
    "            print(f\"\\n  Original dataset has {len(all_columns)} columns:\")\n",
    "            for i, col in enumerate(all_columns, 1):\n",
    "                print(f\"    {i:2d}. {col}\")\n",
    "            \n",
    "            # Display first 5 rows with all columns\n",
    "            print(f\"\\n  First 5 rows of original data (all {len(all_columns)} columns):\")\n",
    "            print(\"-\" * 80)\n",
    "            display(df_full_sample.head(5))\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Step 2: Feature Selection - Define required columns\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"Step 2: Feature Selection\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # Feature Selection: Select only columns needed for analysis\n",
    "            required_columns = [\n",
    "                'pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID',\n",
    "                'trip_miles', 'trip_time', 'base_passenger_fare', 'tips', 'tolls',\n",
    "                'airport_fee', 'driver_pay'\n",
    "            ]\n",
    "            \n",
    "            # Check if all required columns exist\n",
    "            missing_cols = [col for col in required_columns if col not in all_columns]\n",
    "            if missing_cols:\n",
    "                print(f\"\\n  [WARN] Some required columns are missing: {missing_cols}\")\n",
    "                required_columns = [col for col in required_columns if col in all_columns]\n",
    "            \n",
    "            print(f\"\\n  Selected {len(required_columns)} columns for analysis:\")\n",
    "            for i, col in enumerate(required_columns, 1):\n",
    "                print(f\"    {i:2d}. {col}\")\n",
    "            \n",
    "            excluded_cols = [col for col in all_columns if col not in required_columns]\n",
    "            if excluded_cols:\n",
    "                print(f\"\\n  Excluded {len(excluded_cols)} columns (not needed for this analysis):\")\n",
    "                for i, col in enumerate(excluded_cols, 1):\n",
    "                    print(f\"    {i:2d}. {col}\")\n",
    "            \n",
    "            # Display first 5 rows with selected columns only\n",
    "            print(f\"\\n  First 5 rows after feature selection ({len(required_columns)} columns):\")\n",
    "            print(\"-\" * 80)\n",
    "            display(df_full_sample[required_columns].head(5))\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Clean up sample data\n",
    "            del df_full_sample\n",
    "            gc.collect()\n",
    "            \n",
    "            # Step 3: Load all files with selected columns only\n",
    "            # Set sampling ratio (1% to save memory)\n",
    "            SAMPLE_RATIO = 0.01\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"Step 3: Loading {SAMPLE_RATIO*100:.6f}% of data from all {len(parquet_files)} files...\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            all_dfs = []\n",
    "            for i, file_path in enumerate(parquet_files, 1):\n",
    "                print(f\"\\n  Loading file {i}/{len(parquet_files)}: {Path(file_path).name}...\")\n",
    "                df_file = pd.read_parquet(file_path, columns=required_columns)\n",
    "                if len(df_file) > 0:\n",
    "                    df_sampled = df_file.sample(frac=SAMPLE_RATIO, random_state=42)\n",
    "                    all_dfs.append(df_sampled)\n",
    "                    print(f\"    [OK] Loaded {len(df_sampled):,} rows ({SAMPLE_RATIO*100:.6f}% of {len(df_file):,})\")\n",
    "                    del df_file\n",
    "                gc.collect()\n",
    "            \n",
    "            # Merge all dataframes\n",
    "            print(f\"\\nCombining {len(all_dfs)} dataframes...\")\n",
    "            df = pd.concat(all_dfs, ignore_index=True)\n",
    "            del all_dfs\n",
    "            gc.collect()\n",
    "            \n",
    "            print(f\"[OK] DataFrame ready: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "            memory_mb = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "            print(f\"  Memory usage: ~{memory_mb:.6f} MB\")\n",
    "            \n",
    "            # Convert datetime columns\n",
    "            df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "            df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"Data loading complete!\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Failed to load data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        df = None\n",
    "\n",
    "\n",
    "# Data Preview (Simple overview after loading)\n",
    "if 'df' in locals() and df is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Data Loading Summary\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n[OK] Data loaded successfully!\")\n",
    "    print(f\"  Total records: {len(df):,}\")\n",
    "    print(f\"  Total columns: {len(df.columns)}\")\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    print(f\"  Memory usage: ~{memory_mb:.6f} MB\")\n",
    "    \n",
    "    # Column information\n",
    "    print(f\"\\n  Column names ({len(df.columns)}):\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"    {i}. {col}\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\n  Data types:\")\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        print(f\"    {col}: {dtype}\")\n",
    "    \n",
    "    # Simple preview\n",
    "    print(\"\\n[Preview] First 5 records:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Data is ready for analysis in the next sections.\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"[ERROR] Failed to load data. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Quality Assessment\n",
    "\n",
    "## Check for Data Issues\n",
    "\n",
    "Before cleaning the data, we need to identify:\n",
    "\n",
    "- **Missing values**: Check for null/NaN values in critical columns\n",
    "- **Duplicate records**: Identify completely duplicate rows\n",
    "- **Invalid values**: \n",
    "  - Negative fares, negative distances, negative times\n",
    "  - Zero or extremely large values that don't make sense\n",
    "- **Data consistency issues**: \n",
    "  - Dropoff time before pickup time\n",
    "  - Impossible trip characteristics\n",
    "- **Outliers**: Values that are significantly different from the norm\n",
    "\n",
    "This section performs comprehensive data quality assessment to understand what issues need to be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Data Quality Assessment\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "1. Dataset Structure\n",
      "================================================================================\n",
      "Dataset: 2,394,703 rows × 11 columns\n",
      "\n",
      "================================================================================\n",
      "2. Comprehensive Data Quality Analysis\n",
      "================================================================================\n",
      "\n",
      "Dataset Overview:\n",
      "  Total records: 2,394,703\n",
      "  Total columns: 11\n",
      "\n",
      "[1] Missing Values Analysis\n",
      "--------------------------------------------------------------------------------\n",
      "  [OK] No missing values found (100% completeness)\n",
      "\n",
      "[2] Duplicate Records Analysis\n",
      "--------------------------------------------------------------------------------\n",
      "  [OK] No duplicate rows found\n",
      "\n",
      "[3] Negative Values Analysis\n",
      "--------------------------------------------------------------------------------\n",
      "  base_passenger_fare: 166 negative values (0.006932%)\n",
      "    Min value: $-46.280000\n",
      "  driver_pay: 20 negative values (0.000835%)\n",
      "    Min value: $-46.940000\n",
      "\n",
      "[4] Zero Values Analysis (Potential Issues)\n",
      "--------------------------------------------------------------------------------\n",
      "  trip_miles: 378 zero values (0.015785%)\n",
      "  trip_time: 1 zero values (0.000042%)\n",
      "  base_passenger_fare: 537 zero values (0.022424%)\n",
      "\n",
      "[5] Time Consistency Analysis\n",
      "--------------------------------------------------------------------------------\n",
      "  [WARN] Datetime columns not properly converted\n",
      "\n",
      "[6] Extreme Outliers Analysis\n",
      "--------------------------------------------------------------------------------\n",
      "  trip_miles > 100 miles: 208 trips (0.008686%)\n",
      "    Max value: 228.070000 miles\n",
      "\n",
      "================================================================================\n",
      "3. Field Type Analysis\n",
      "================================================================================\n",
      "No categorical fields\n",
      "\n",
      "================================================================================\n",
      "4. Data Sample Preview\n",
      "================================================================================\n",
      "\n",
      "Sample (11 columns):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>base_passenger_fare</th>\n",
       "      <th>tips</th>\n",
       "      <th>tolls</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>driver_pay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-09 22:11:03</td>\n",
       "      <td>2024-01-09 22:29:46</td>\n",
       "      <td>230</td>\n",
       "      <td>112</td>\n",
       "      <td>5.466</td>\n",
       "      <td>1123</td>\n",
       "      <td>26.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-26 08:07:17</td>\n",
       "      <td>2024-01-26 08:35:38</td>\n",
       "      <td>85</td>\n",
       "      <td>77</td>\n",
       "      <td>4.290</td>\n",
       "      <td>1701</td>\n",
       "      <td>27.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-19 02:17:05</td>\n",
       "      <td>2024-01-19 02:29:12</td>\n",
       "      <td>220</td>\n",
       "      <td>243</td>\n",
       "      <td>2.550</td>\n",
       "      <td>727</td>\n",
       "      <td>15.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-21 01:44:00</td>\n",
       "      <td>2024-01-21 02:08:30</td>\n",
       "      <td>164</td>\n",
       "      <td>80</td>\n",
       "      <td>6.370</td>\n",
       "      <td>1470</td>\n",
       "      <td>24.57</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 14:19:17</td>\n",
       "      <td>2024-01-01 14:28:14</td>\n",
       "      <td>260</td>\n",
       "      <td>82</td>\n",
       "      <td>1.681</td>\n",
       "      <td>537</td>\n",
       "      <td>12.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_datetime    dropoff_datetime  PULocationID  DOLocationID  \\\n",
       "0 2024-01-09 22:11:03 2024-01-09 22:29:46           230           112   \n",
       "1 2024-01-26 08:07:17 2024-01-26 08:35:38            85            77   \n",
       "2 2024-01-19 02:17:05 2024-01-19 02:29:12           220           243   \n",
       "3 2024-01-21 01:44:00 2024-01-21 02:08:30           164            80   \n",
       "4 2024-01-01 14:19:17 2024-01-01 14:28:14           260            82   \n",
       "\n",
       "   trip_miles  trip_time  base_passenger_fare  tips  tolls  airport_fee  \\\n",
       "0       5.466       1123                26.48  0.00    5.8          0.0   \n",
       "1       4.290       1701                27.49  0.00    0.0          0.0   \n",
       "2       2.550        727                15.14  0.00    0.0          0.0   \n",
       "3       6.370       1470                24.57  3.01    0.0          0.0   \n",
       "4       1.681        537                12.32  0.00    0.0          0.0   \n",
       "\n",
       "   driver_pay  \n",
       "0       20.88  \n",
       "1       24.88  \n",
       "2       10.19  \n",
       "3       23.00  \n",
       "4        7.32  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Detailed Quality Checks\n",
      "================================================================================\n",
      "\n",
      "[7] Detailed Negative Fares Analysis\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  base_passenger_fare:\n",
      "    Count: 166 (0.0069%)\n",
      "    Min value: $-46.280000\n",
      "    Max value: $-0.010000\n",
      "    Mean: $-5.752169\n",
      "    Median: $-3.745000\n",
      "    Sample values: [-46.28, -43.11, -36.25, -32.18, -26.07, -25.47, -24.1, -23.22, -22.67, -20.0]...\n",
      "\n",
      "  driver_pay:\n",
      "    Count: 20 (0.0008%)\n",
      "    Min value: $-46.940000\n",
      "    Max value: $-1.060000\n",
      "    Mean: $-16.632500\n",
      "    Median: $-10.000000\n",
      "    All values: [-46.94, -37.56, -34.87, -33.88, -30.22, -20.82, -20.0, -19.6, -13.06, -6.94, -6.65, -6.42, -5.2, -4.38, -4.23, -1.06]\n",
      "\n",
      "[8] Invalid Time Order Detailed Analysis\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[9] Data Range Analysis\n",
      "--------------------------------------------------------------------------------\n",
      "  Date Range:\n",
      "    Earliest pickup: 2024-01-01 00:00:50\n",
      "    Latest pickup: 2024-12-31 23:59:16\n",
      "    Date span: 365 days\n",
      "\n",
      "  Trip Distance Range:\n",
      "    Min: 0.0000 miles\n",
      "    Max: 228.0700 miles\n",
      "    Mean: 5.084194 miles\n",
      "    Median: 3.014000 miles\n",
      "    25th percentile: 1.570000 miles\n",
      "    75th percentile: 6.380000 miles\n",
      "    95th percentile: 16.600000 miles\n",
      "    99th percentile: 27.334980 miles\n",
      "\n",
      "  Trip Duration Range:\n",
      "    Min: 0.000000 seconds (0.000000 minutes)\n",
      "    Max: 35881.000000 seconds (9.966944 hours)\n",
      "    Mean: 1207.889050 seconds (20.131484 minutes)\n",
      "    Median: 979.000000 seconds (16.316667 minutes)\n",
      "    25th percentile: 604.000000 seconds (10.066667 minutes)\n",
      "    75th percentile: 1555.000000 seconds (25.916667 minutes)\n",
      "    95th percentile: 2889.000000 seconds (48.150000 minutes)\n",
      "    99th percentile: 4288.000000 seconds (71.466667 minutes)\n",
      "\n",
      "  Fare Range:\n",
      "    Min: $-46.280000\n",
      "    Max: $926.430000\n",
      "    Mean: $26.224095\n",
      "    Median: $19.610000\n",
      "    25th percentile: $12.490000\n",
      "    75th percentile: $31.390000\n",
      "    95th percentile: $67.100000\n",
      "    99th percentile: $117.900000\n",
      "\n",
      "[10] Location ID Analysis\n",
      "--------------------------------------------------------------------------------\n",
      "  Unique Pickup Locations: 260\n",
      "  Pickup Location Range: 2 to 265\n",
      "  Unique Dropoff Locations: 261\n",
      "  Dropoff Location Range: 1 to 265\n",
      "\n",
      "================================================================================\n",
      "Data Quality Summary - All Issues Detected\n",
      "================================================================================\n",
      "\n",
      "[3] Negative Values Summary:\n",
      "  ----------------------------------------------------------------------------\n",
      "  Negative fare values: 186 (0.007767%)\n",
      "\n",
      "[4] Zero Values Summary:\n",
      "  ----------------------------------------------------------------------------\n",
      "    - trip_miles: 378 (0.015785%)\n",
      "    - trip_time: 1 (0.000042%)\n",
      "    - base_passenger_fare: 537 (0.022424%)\n",
      "\n",
      "[5] Time Consistency Check:\n",
      "  ----------------------------------------------------------------------------\n",
      "  [WARN] Datetime columns not properly converted - cannot check time order\n",
      "\n",
      "[6] Extreme Outliers Summary:\n",
      "  ----------------------------------------------------------------------------\n",
      "  trip_miles > 100 miles: 208 (0.008686%), Max: 228.070000 miles\n",
      "\n",
      "================================================================================\n",
      "Cleaning Actions Required\n",
      "================================================================================\n",
      "\n",
      "  Total issues found: 3\n",
      "  ----------------------------------------------------------------------------\n",
      "    1. Negative fare values (186) → [CLEANING: SET TO 0]\n",
      "    2. Zero trip_miles (378) → [NOTE: KEEP for analysis]\n",
      "    3. Extreme trip_miles >100 miles (208) → [CLEANING: DELETE]\n",
      "\n",
      "  Overall Data Completeness: 100.000000%\n",
      "  Total records: 2,394,703\n",
      "  Total columns: 11\n",
      "\n",
      "================================================================================\n",
      "Data quality assessment complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Data Quality Assessment\n",
    "if 'df' in locals() and df is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Data Quality Assessment\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Dataset Structure\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"1. Dataset Structure\")\n",
    "    print(\"=\"*80)\n",
    "    analyze_dataset_structure(df)\n",
    "    \n",
    "    # 2. Comprehensive Data Quality Analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"2. Comprehensive Data Quality Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    analyze_data_quality(df)\n",
    "    \n",
    "    # 3. Field Type Analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"3. Field Type Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    analyze_categorical_fields(df)\n",
    "    \n",
    "    # 4. Data Sample Preview\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"4. Data Sample Preview\")\n",
    "    print(\"=\"*80)\n",
    "    sample_data_display(df)\n",
    "    \n",
    "    # ===== Detailed Quality Checks =====\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Detailed Quality Checks\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # 1. Detailed Negative Fares Breakdown (Detailed version of [3])\n",
    "    print(\"\\n[7] Detailed Negative Fares Analysis\")\n",
    "    print(\"-\" * 80)\n",
    "    fare_cols = ['base_passenger_fare', 'tips', 'tolls', 'airport_fee', 'driver_pay']\n",
    "    for col in fare_cols:\n",
    "        if col in df.columns:\n",
    "            negative_count = (df[col] < 0).sum()\n",
    "            if negative_count > 0:\n",
    "                negative_pct = (negative_count / total_rows * 100)\n",
    "                negative_data = df[df[col] < 0][col]\n",
    "                print(f\"\\n  {col}:\")\n",
    "                print(f\"    Count: {negative_count:,} ({negative_pct:.4f}%)\")\n",
    "                print(f\"    Min value: ${negative_data.min():.6f}\")\n",
    "                print(f\"    Max value: ${negative_data.max():.6f}\")\n",
    "                print(f\"    Mean: ${negative_data.mean():.6f}\")\n",
    "                print(f\"    Median: ${negative_data.median():.6f}\")\n",
    "                # Show distribution\n",
    "                if negative_count <= 20:\n",
    "                    print(f\"    All values: {sorted(negative_data.unique())}\")\n",
    "                else:\n",
    "                    print(f\"    Sample values: {sorted(negative_data.unique())[:10]}...\")\n",
    "    \n",
    "    # 2. Invalid Time Order Details (Detailed version of [5])\n",
    "    print(\"\\n[8] Invalid Time Order Detailed Analysis\")\n",
    "    print(\"-\" * 80)\n",
    "    if 'pickup_datetime' in df.columns and 'dropoff_datetime' in df.columns:\n",
    "        if df['pickup_datetime'].dtype == 'datetime64[ns]' and df['dropoff_datetime'].dtype == 'datetime64[ns]':\n",
    "            invalid_mask = df['dropoff_datetime'] < df['pickup_datetime']\n",
    "            invalid_count = invalid_mask.sum()\n",
    "            if invalid_count > 0:\n",
    "                invalid_data = df[invalid_mask]\n",
    "                time_diff = (invalid_data['pickup_datetime'] - invalid_data['dropoff_datetime']).dt.total_seconds()\n",
    "                print(f\"  Total invalid records: {invalid_count:,}\")\n",
    "                print(f\"  Time difference statistics:\")\n",
    "                print(f\"    Min difference: {time_diff.min():.6f} seconds ({time_diff.min()/3600:.6f} hours)\")\n",
    "                print(f\"    Max difference: {time_diff.max():.6f} seconds ({time_diff.max()/3600:.6f} hours)\")\n",
    "                print(f\"    Mean difference: {time_diff.mean():.6f} seconds ({time_diff.mean()/3600:.6f} hours)\")\n",
    "                print(f\"    Median difference: {time_diff.median():.6f} seconds ({time_diff.median()/3600:.6f} hours)\")\n",
    "                \n",
    "                # Show sample records\n",
    "                if invalid_count <= 10:\n",
    "                    print(f\"\\n  All invalid records:\")\n",
    "                    display(invalid_data[['pickup_datetime', 'dropoff_datetime', 'trip_miles', 'trip_time']])\n",
    "                else:\n",
    "                    print(f\"\\n  Sample invalid records (first 5):\")\n",
    "                    display(invalid_data[['pickup_datetime', 'dropoff_datetime', 'trip_miles', 'trip_time']].head(5))\n",
    "    \n",
    "    # 3. Data Range Analysis\n",
    "    print(\"\\n[9] Data Range Analysis\")\n",
    "    print(\"-\" * 80)\n",
    "    if 'pickup_datetime' in df.columns:\n",
    "        print(f\"  Date Range:\")\n",
    "        print(f\"    Earliest pickup: {df['pickup_datetime'].min()}\")\n",
    "        print(f\"    Latest pickup: {df['pickup_datetime'].max()}\")\n",
    "        print(f\"    Date span: {(df['pickup_datetime'].max() - df['pickup_datetime'].min()).days} days\")\n",
    "    \n",
    "    if 'trip_miles' in df.columns:\n",
    "        print(f\"\\n  Trip Distance Range:\")\n",
    "        print(f\"    Min: {df['trip_miles'].min():.4f} miles\")\n",
    "        print(f\"    Max: {df['trip_miles'].max():.4f} miles\")\n",
    "        print(f\"    Mean: {df['trip_miles'].mean():.6f} miles\")\n",
    "        print(f\"    Median: {df['trip_miles'].median():.6f} miles\")\n",
    "        print(f\"    25th percentile: {df['trip_miles'].quantile(0.25):.6f} miles\")\n",
    "        print(f\"    75th percentile: {df['trip_miles'].quantile(0.75):.6f} miles\")\n",
    "        print(f\"    95th percentile: {df['trip_miles'].quantile(0.95):.6f} miles\")\n",
    "        print(f\"    99th percentile: {df['trip_miles'].quantile(0.99):.6f} miles\")\n",
    "    \n",
    "    if 'trip_time' in df.columns:\n",
    "        print(f\"\\n  Trip Duration Range:\")\n",
    "        print(f\"    Min: {df['trip_time'].min():.6f} seconds ({df['trip_time'].min()/60:.6f} minutes)\")\n",
    "        print(f\"    Max: {df['trip_time'].max():.6f} seconds ({df['trip_time'].max()/3600:.6f} hours)\")\n",
    "        print(f\"    Mean: {df['trip_time'].mean():.6f} seconds ({df['trip_time'].mean()/60:.6f} minutes)\")\n",
    "        print(f\"    Median: {df['trip_time'].median():.6f} seconds ({df['trip_time'].median()/60:.6f} minutes)\")\n",
    "        print(f\"    25th percentile: {df['trip_time'].quantile(0.25):.6f} seconds ({df['trip_time'].quantile(0.25)/60:.6f} minutes)\")\n",
    "        print(f\"    75th percentile: {df['trip_time'].quantile(0.75):.6f} seconds ({df['trip_time'].quantile(0.75)/60:.6f} minutes)\")\n",
    "        print(f\"    95th percentile: {df['trip_time'].quantile(0.95):.6f} seconds ({df['trip_time'].quantile(0.95)/60:.6f} minutes)\")\n",
    "        print(f\"    99th percentile: {df['trip_time'].quantile(0.99):.6f} seconds ({df['trip_time'].quantile(0.99)/60:.6f} minutes)\")\n",
    "    \n",
    "    if 'base_passenger_fare' in df.columns:\n",
    "        print(f\"\\n  Fare Range:\")\n",
    "        print(f\"    Min: ${df['base_passenger_fare'].min():.6f}\")\n",
    "        print(f\"    Max: ${df['base_passenger_fare'].max():.6f}\")\n",
    "        print(f\"    Mean: ${df['base_passenger_fare'].mean():.6f}\")\n",
    "        print(f\"    Median: ${df['base_passenger_fare'].median():.6f}\")\n",
    "        print(f\"    25th percentile: ${df['base_passenger_fare'].quantile(0.25):.6f}\")\n",
    "        print(f\"    75th percentile: ${df['base_passenger_fare'].quantile(0.75):.6f}\")\n",
    "        print(f\"    95th percentile: ${df['base_passenger_fare'].quantile(0.95):.6f}\")\n",
    "        print(f\"    99th percentile: ${df['base_passenger_fare'].quantile(0.99):.6f}\")\n",
    "    \n",
    "    # 4. Location ID Analysis\n",
    "    print(\"\\n[10] Location ID Analysis\")\n",
    "    print(\"-\" * 80)\n",
    "    if 'PULocationID' in df.columns:\n",
    "        unique_pu = df['PULocationID'].nunique()\n",
    "        print(f\"  Unique Pickup Locations: {unique_pu:,}\")\n",
    "        print(f\"  Pickup Location Range: {df['PULocationID'].min()} to {df['PULocationID'].max()}\")\n",
    "        # Check for invalid location IDs (0 or negative)\n",
    "        invalid_pu = (df['PULocationID'] <= 0).sum()\n",
    "        if invalid_pu > 0:\n",
    "            print(f\"  [WARN] Invalid Pickup Location IDs (<=0): {invalid_pu:,} ({invalid_pu/total_rows*100:.4f}%)\")\n",
    "    \n",
    "    if 'DOLocationID' in df.columns:\n",
    "        unique_do = df['DOLocationID'].nunique()\n",
    "        print(f\"  Unique Dropoff Locations: {unique_do:,}\")\n",
    "        print(f\"  Dropoff Location Range: {df['DOLocationID'].min()} to {df['DOLocationID'].max()}\")\n",
    "        # Check for invalid location IDs (0 or negative)\n",
    "        invalid_do = (df['DOLocationID'] <= 0).sum()\n",
    "        if invalid_do > 0:\n",
    "            print(f\"  [WARN] Invalid Dropoff Location IDs (<=0): {invalid_do:,} ({invalid_do/total_rows*100:.4f}%)\")\n",
    "    \n",
    "\n",
    "\n",
    "    # ===== Comprehensive Summary (After All Checks) =====\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Data Quality Summary - All Issues Detected\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Collect all issues with detailed information\n",
    "    cleaning_actions = []\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # 1. Missing Values - Detailed\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"\\n[1] Missing Values Summary:\")\n",
    "        print(f\"  {'-' * 76}\")\n",
    "        missing_count = missing.sum()\n",
    "        missing_pct_total = (missing_count / (total_rows * len(df.columns)) * 100)\n",
    "        print(f\"  Total missing values: {missing_count:,} ({missing_pct_total:.6f}% of all cells)\")\n",
    "        \n",
    "        # Group by category\n",
    "        critical_cols = ['pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID']\n",
    "        fare_cols_list = ['base_passenger_fare', 'tips', 'tolls', 'airport_fee', 'driver_pay']\n",
    "        trip_cols = ['trip_miles', 'trip_time']\n",
    "        \n",
    "        critical_missing = 0\n",
    "        fare_missing = 0\n",
    "        trip_missing = 0\n",
    "        other_missing = 0\n",
    "        \n",
    "        for col in missing[missing > 0].index:\n",
    "            col_missing = missing[col]\n",
    "            col_pct = (col_missing / total_rows * 100)\n",
    "            print(f\"    - {col}: {col_missing:,} ({col_pct:.6f}%)\")\n",
    "            \n",
    "            if col in critical_cols:\n",
    "                critical_missing += col_missing\n",
    "            elif col in fare_cols_list:\n",
    "                fare_missing += col_missing\n",
    "            elif col in trip_cols:\n",
    "                trip_missing += col_missing\n",
    "            else:\n",
    "                other_missing += col_missing\n",
    "        \n",
    "        if critical_missing > 0:\n",
    "            cleaning_actions.append(f\"Missing critical fields ({critical_missing:,} records) → [CLEANING: DELETE]\")\n",
    "        if fare_missing > 0:\n",
    "            cleaning_actions.append(f\"Missing fare fields ({fare_missing:,} records) → [CLEANING: FILL 0]\")\n",
    "        if trip_missing > 0:\n",
    "            cleaning_actions.append(f\"Missing trip data ({trip_missing:,} records) → [CLEANING: DELETE]\")\n",
    "        if other_missing > 0:\n",
    "            cleaning_actions.append(f\"Missing other fields ({other_missing:,} records) → [CLEANING: REVIEW]\")\n",
    "    \n",
    "    # 2. Duplicate Records\n",
    "    full_duplicates = df.duplicated().sum()\n",
    "    if full_duplicates > 0:\n",
    "        dup_pct = (full_duplicates / total_rows * 100)\n",
    "        print(f\"\\n[2] Duplicate Records Summary:\")\n",
    "        print(f\"  {'-' * 76}\")\n",
    "        print(f\"  Total duplicate rows: {full_duplicates:,} ({dup_pct:.6f}%)\")\n",
    "        cleaning_actions.append(f\"Duplicate records ({full_duplicates:,}) → [CLEANING: DELETE]\")\n",
    "    \n",
    "    # 3. Negative Values - Detailed\n",
    "    negative_fare_total = 0\n",
    "    negative_miles = 0\n",
    "    negative_time = 0\n",
    "    \n",
    "    fare_cols_list = ['base_passenger_fare', 'tips', 'tolls', 'airport_fee', 'driver_pay']\n",
    "    for col in fare_cols_list:\n",
    "        if col in df.columns:\n",
    "            neg_count = (df[col] < 0).sum()\n",
    "            if neg_count > 0:\n",
    "                negative_fare_total += neg_count\n",
    "    \n",
    "    if 'trip_miles' in df.columns:\n",
    "        negative_miles = (df['trip_miles'] < 0).sum()\n",
    "    if 'trip_time' in df.columns:\n",
    "        negative_time = (df['trip_time'] < 0).sum()\n",
    "    \n",
    "    if negative_fare_total > 0 or negative_miles > 0 or negative_time > 0:\n",
    "        print(f\"\\n[3] Negative Values Summary:\")\n",
    "        print(f\"  {'-' * 76}\")\n",
    "        if negative_fare_total > 0:\n",
    "            neg_pct = (negative_fare_total / total_rows * 100)\n",
    "            print(f\"  Negative fare values: {negative_fare_total:,} ({neg_pct:.6f}%)\")\n",
    "            cleaning_actions.append(f\"Negative fare values ({negative_fare_total:,}) → [CLEANING: SET TO 0]\")\n",
    "        if negative_miles > 0:\n",
    "            neg_pct = (negative_miles / total_rows * 100)\n",
    "            print(f\"  Negative trip_miles: {negative_miles:,} ({neg_pct:.6f}%)\")\n",
    "            cleaning_actions.append(f\"Negative trip_miles ({negative_miles:,}) → [CLEANING: DELETE]\")\n",
    "        if negative_time > 0:\n",
    "            neg_pct = (negative_time / total_rows * 100)\n",
    "            print(f\"  Negative trip_time: {negative_time:,} ({neg_pct:.6f}%)\")\n",
    "            cleaning_actions.append(f\"Negative trip_time ({negative_time:,}) → [CLEANING: DELETE]\")\n",
    "    \n",
    "    # 4. Zero Values - Detailed\n",
    "    zero_issues_list = []\n",
    "    if 'trip_miles' in df.columns:\n",
    "        zero_miles = (df['trip_miles'] == 0).sum()\n",
    "        if zero_miles > 0:\n",
    "            zero_pct = (zero_miles / total_rows * 100)\n",
    "            zero_issues_list.append(f\"trip_miles: {zero_miles:,} ({zero_pct:.6f}%)\")\n",
    "    \n",
    "    if 'trip_time' in df.columns:\n",
    "        zero_time = (df['trip_time'] == 0).sum()\n",
    "        if zero_time > 0:\n",
    "            zero_pct = (zero_time / total_rows * 100)\n",
    "            zero_issues_list.append(f\"trip_time: {zero_time:,} ({zero_pct:.6f}%)\")\n",
    "    \n",
    "    if 'base_passenger_fare' in df.columns:\n",
    "        zero_fare = (df['base_passenger_fare'] == 0).sum()\n",
    "        if zero_fare > 0:\n",
    "            zero_pct = (zero_fare / total_rows * 100)\n",
    "            zero_issues_list.append(f\"base_passenger_fare: {zero_fare:,} ({zero_pct:.6f}%)\")\n",
    "    \n",
    "    if zero_issues_list:\n",
    "        print(f\"\\n[4] Zero Values Summary:\")\n",
    "        print(f\"  {'-' * 76}\")\n",
    "        for issue in zero_issues_list:\n",
    "            print(f\"    - {issue}\")\n",
    "        if 'trip_miles' in df.columns and (df['trip_miles'] == 0).sum() > 0:\n",
    "            zero_miles = (df['trip_miles'] == 0).sum()\n",
    "            cleaning_actions.append(f\"Zero trip_miles ({zero_miles:,}) → [NOTE: KEEP for analysis]\")\n",
    "    \n",
    "    # 5. Invalid Time Order\n",
    "    invalid_order = 0\n",
    "    if 'pickup_datetime' in df.columns and 'dropoff_datetime' in df.columns:\n",
    "        if df['pickup_datetime'].dtype == 'datetime64[ns]' and df['dropoff_datetime'].dtype == 'datetime64[ns]':\n",
    "            invalid_order = (df['dropoff_datetime'] < df['pickup_datetime']).sum()\n",
    "            if invalid_order > 0:\n",
    "                invalid_pct = (invalid_order / total_rows * 100)\n",
    "                print(f\"\\n[5] Invalid Time Order Summary:\")\n",
    "                print(f\"  {'-' * 76}\")\n",
    "                print(f\"  Invalid time order (dropoff < pickup): {invalid_order:,} ({invalid_pct:.6f}%)\")\n",
    "                cleaning_actions.append(f\"Invalid time order ({invalid_order:,}) → [CLEANING: DELETE]\")\n",
    "        else:\n",
    "            print(f\"\\n[5] Time Consistency Check:\")\n",
    "            print(f\"  {'-' * 76}\")\n",
    "            print(f\"  [WARN] Datetime columns not properly converted - cannot check time order\")\n",
    "    \n",
    "    # 6. Extreme Outliers - Detailed\n",
    "    extreme_miles = 0\n",
    "    extreme_time = 0\n",
    "    extreme_fare = 0\n",
    "    \n",
    "    if 'trip_miles' in df.columns:\n",
    "        extreme_miles = (df['trip_miles'] > 100).sum()\n",
    "    if 'trip_time' in df.columns:\n",
    "        extreme_time = (df['trip_time'] > 24 * 3600).sum()\n",
    "    if 'base_passenger_fare' in df.columns:\n",
    "        extreme_fare = (df['base_passenger_fare'] > 1000).sum()\n",
    "    \n",
    "    if extreme_miles > 0 or extreme_time > 0 or extreme_fare > 0:\n",
    "        print(f\"\\n[6] Extreme Outliers Summary:\")\n",
    "        print(f\"  {'-' * 76}\")\n",
    "        if extreme_miles > 0:\n",
    "            extreme_pct = (extreme_miles / total_rows * 100)\n",
    "            max_miles = df['trip_miles'].max()\n",
    "            print(f\"  trip_miles > 100 miles: {extreme_miles:,} ({extreme_pct:.6f}%), Max: {max_miles:.6f} miles\")\n",
    "            cleaning_actions.append(f\"Extreme trip_miles >100 miles ({extreme_miles:,}) → [CLEANING: DELETE]\")\n",
    "        if extreme_time > 0:\n",
    "            extreme_pct = (extreme_time / total_rows * 100)\n",
    "            max_time = df['trip_time'].max()\n",
    "            print(f\"  trip_time > 24 hours: {extreme_time:,} ({extreme_pct:.6f}%), Max: {max_time:.6f} hours\")\n",
    "            cleaning_actions.append(f\"Extreme trip_time >24h ({extreme_time:,}) → [CLEANING: DELETE]\")\n",
    "        if extreme_fare > 0:\n",
    "            extreme_pct = (extreme_fare / total_rows * 100)\n",
    "            max_fare = df['base_passenger_fare'].max()\n",
    "            print(f\"  base_passenger_fare > $1000: {extreme_fare:,} ({extreme_pct:.6f}%), Max: ${max_fare:.6f}\")\n",
    "            cleaning_actions.append(f\"Extreme fare >$1000 ({extreme_fare:,}) → [CLEANING: REVIEW]\")\n",
    "    \n",
    "    # Overall Summary\n",
    "    completeness = ((total_rows * len(df.columns) - missing.sum()) / (total_rows * len(df.columns)) * 100) if missing.sum() > 0 else 100.0\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"Cleaning Actions Required\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if cleaning_actions:\n",
    "        print(f\"\\n  Total issues found: {len(cleaning_actions)}\")\n",
    "        print(f\"  {'-' * 76}\")\n",
    "        for i, action in enumerate(cleaning_actions, 1):\n",
    "            print(f\"    {i}. {action}\")\n",
    "    else:\n",
    "        print(f\"\\n  [OK] No data quality issues detected!\")\n",
    "        print(f\"  No cleaning actions required.\")\n",
    "    \n",
    "    print(f\"\\n  Overall Data Completeness: {completeness:.6f}%\")\n",
    "    print(f\"  Total records: {total_rows:,}\")\n",
    "    print(f\"  Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Data quality assessment complete!\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning\n",
    "\n",
    "## Fix and Clean the Data\n",
    "\n",
    "Based on the data quality assessment, we will:\n",
    "\n",
    "1. **Handle missing values**: Remove rows with missing critical data, fill missing fare components with 0\n",
    "2. **Fix negative values**: Set negative fares and other invalid values to 0\n",
    "3. **Remove invalid trips**: \n",
    "   - Trips with negative distance or time\n",
    "   - Trips with dropoff before pickup\n",
    "   - Extremely long trips (> 24 hours)\n",
    "4. **Remove duplicates**: Remove completely duplicate records\n",
    "\n",
    "**Note**: We perform data cleaning AFTER the quality assessment to understand the issues first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Data Cleaning\n",
      "================================================================================\n",
      "\n",
      "Original dataset: 2,394,703 records\n",
      "\n",
      "[0] Handling Missing Values...\n",
      "  [OK] No missing values found\n",
      "\n",
      "[1] Handling Negative Fares...\n",
      "  Found 166 negative values in base_passenger_fare\n",
      "  Found 20 negative values in driver_pay\n",
      "\n",
      "[2] [OK] No trips with negative distance found\n",
      "\n",
      "[3] [OK] No trips with negative time found\n",
      "\n",
      "[4] Removing trips with dropoff before pickup: 90 records\n",
      "\n",
      "[6] [OK] No duplicate rows found\n",
      "\n",
      "[7] [OK] No trips longer than 24 hours found\n",
      "\n",
      "[8] Removing trips longer than 100 miles: 208 records\n",
      "\n",
      "[9] [OK] No trips with fare > $1000 found\n",
      "\n",
      "================================================================================\n",
      "Cleaning Summary\n",
      "================================================================================\n",
      "\n",
      "Original records: 2,394,703\n",
      "Cleaned records: 2,394,405\n",
      "Records removed: 298 (0.012444%)\n",
      "Records retained: 99.987556%\n",
      "\n",
      "Cleaning steps performed:\n",
      "  1. Set 166 negative base_passenger_fare values to 0\n",
      "  2. Set 20 negative driver_pay values to 0\n",
      "  3. Removed 90 trips with invalid time order\n",
      "  4. Removed 208 trips longer than 100 miles\n",
      "\n",
      "[OK] Data cleaning complete! Using cleaned dataset with 2,394,405 records for further analysis.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning based on EDA findings\n",
    "if 'df' in locals() and df is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Data Cleaning\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Store original data size\n",
    "    original_count = len(df)\n",
    "    print(f\"\\nOriginal dataset: {original_count:,} records\")\n",
    "    \n",
    "    # Create a copy for cleaning\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    cleaning_steps = []\n",
    "    \n",
    "    # 0. Handle missing values (based on EDA findings)\n",
    "    print(\"\\n[0] Handling Missing Values...\")\n",
    "    missing_before = df_cleaned.isnull().sum().sum()\n",
    "    if missing_before > 0:\n",
    "        print(f\"  Total missing values found: {missing_before:,}\")\n",
    "        \n",
    "        # Handle missing values in critical columns\n",
    "        critical_cols = ['pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID']\n",
    "        for col in critical_cols:\n",
    "            if col in df_cleaned.columns:\n",
    "                missing_count = df_cleaned[col].isnull().sum()\n",
    "                if missing_count > 0:\n",
    "                    print(f\"    Removing {missing_count:,} rows with missing {col}\")\n",
    "                    df_cleaned = df_cleaned.dropna(subset=[col])\n",
    "                    cleaning_steps.append(f\"Removed {missing_count:,} rows with missing {col}\")\n",
    "        \n",
    "        # For fare columns, fill missing with 0 (assuming no fare means 0)\n",
    "        fare_cols = ['base_passenger_fare', 'tips', 'tolls', 'airport_fee', 'driver_pay']\n",
    "        for col in fare_cols:\n",
    "            if col in df_cleaned.columns:\n",
    "                missing_count = df_cleaned[col].isnull().sum()\n",
    "                if missing_count > 0:\n",
    "                    print(f\"    Filling {missing_count:,} missing {col} values with 0\")\n",
    "                    df_cleaned[col] = df_cleaned[col].fillna(0)\n",
    "                    cleaning_steps.append(f\"Filled {missing_count:,} missing {col} values with 0\")\n",
    "        \n",
    "        # For numeric trip fields, remove rows with missing values\n",
    "        numeric_trip_cols = ['trip_miles', 'trip_time']\n",
    "        for col in numeric_trip_cols:\n",
    "            if col in df_cleaned.columns:\n",
    "                missing_count = df_cleaned[col].isnull().sum()\n",
    "                if missing_count > 0:\n",
    "                    print(f\"    Removing {missing_count:,} rows with missing {col}\")\n",
    "                    df_cleaned = df_cleaned.dropna(subset=[col])\n",
    "                    cleaning_steps.append(f\"Removed {missing_count:,} rows with missing {col}\")\n",
    "        \n",
    "        missing_after = df_cleaned.isnull().sum().sum()\n",
    "        if missing_after == 0:\n",
    "            print(f\"  [OK] All missing values handled\")\n",
    "        else:\n",
    "            print(f\"  [WARN] Remaining missing values: {missing_after:,}\")\n",
    "    else:\n",
    "        print(\"  [OK] No missing values found\")\n",
    "    \n",
    "    # 1. Handle negative fares (based on EDA findings)\n",
    "    print(\"\\n[1] Handling Negative Fares...\")\n",
    "    fare_cols = ['base_passenger_fare', 'tips', 'tolls', 'airport_fee', 'driver_pay']\n",
    "    \n",
    "    found_negative = False\n",
    "    for col in fare_cols:\n",
    "        if col in df_cleaned.columns:\n",
    "            negative_count = (df_cleaned[col] < 0).sum()\n",
    "            if negative_count > 0:\n",
    "                found_negative = True\n",
    "                print(f\"  Found {negative_count:,} negative values in {col}\")\n",
    "                # Option 1: Set negative values to 0\n",
    "                df_cleaned.loc[df_cleaned[col] < 0, col] = 0\n",
    "                cleaning_steps.append(f\"Set {negative_count:,} negative {col} values to 0\")\n",
    "                # Alternative: Could also remove these rows\n",
    "                # df_cleaned = df_cleaned[df_cleaned[col] >= 0]\n",
    "    \n",
    "    if not found_negative:\n",
    "        print(\"  [OK] No negative fare values found\")\n",
    "    \n",
    "    # 2. Remove negative trip distances\n",
    "    if 'trip_miles' in df_cleaned.columns:\n",
    "        negative_miles = (df_cleaned['trip_miles'] < 0).sum()\n",
    "        if negative_miles > 0:\n",
    "            print(f\"\\n[2] Removing trips with negative distance: {negative_miles:,} records\")\n",
    "            df_cleaned = df_cleaned[df_cleaned['trip_miles'] >= 0]\n",
    "            cleaning_steps.append(f\"Removed {negative_miles:,} trips with negative distance\")\n",
    "        else:\n",
    "            print(f\"\\n[2] [OK] No trips with negative distance found\")\n",
    "    else:\n",
    "        print(f\"\\n[2] [SKIP] trip_miles column not found\")\n",
    "    \n",
    "    # 3. Remove negative trip times\n",
    "    if 'trip_time' in df_cleaned.columns:\n",
    "        negative_time = (df_cleaned['trip_time'] < 0).sum()\n",
    "        if negative_time > 0:\n",
    "            print(f\"\\n[3] Removing trips with negative time: {negative_time:,} records\")\n",
    "            df_cleaned = df_cleaned[df_cleaned['trip_time'] >= 0]\n",
    "            cleaning_steps.append(f\"Removed {negative_time:,} trips with negative time\")\n",
    "        else:\n",
    "            print(f\"\\n[3] [OK] No trips with negative time found\")\n",
    "    else:\n",
    "        print(f\"\\n[3] [SKIP] trip_time column not found\")\n",
    "    \n",
    "    # 4. Remove trips with invalid time order (dropoff before pickup)\n",
    "    if 'pickup_datetime' in df_cleaned.columns and 'dropoff_datetime' in df_cleaned.columns:\n",
    "        # Check if datetime columns are properly converted\n",
    "        if (pd.api.types.is_datetime64_any_dtype(df_cleaned['pickup_datetime']) and \n",
    "            pd.api.types.is_datetime64_any_dtype(df_cleaned['dropoff_datetime'])):\n",
    "            invalid_order = (df_cleaned['dropoff_datetime'] < df_cleaned['pickup_datetime']).sum()\n",
    "            if invalid_order > 0:\n",
    "                print(f\"\\n[4] Removing trips with dropoff before pickup: {invalid_order:,} records\")\n",
    "                df_cleaned = df_cleaned[df_cleaned['dropoff_datetime'] >= df_cleaned['pickup_datetime']]\n",
    "                cleaning_steps.append(f\"Removed {invalid_order:,} trips with invalid time order\")\n",
    "            else:\n",
    "                print(f\"\\n[4] [OK] No trips with invalid time order found\")\n",
    "        else:\n",
    "            print(f\"\\n[4] [WARN] Datetime columns not properly converted, skipping time order check\")\n",
    "    else:\n",
    "        print(f\"\\n[4] [SKIP] Datetime columns not found\")\n",
    "    \n",
    "    # 5. Remove zero-distance trips (optional - you might want to keep these)\n",
    "    if 'trip_miles' in df_cleaned.columns:\n",
    "        zero_distance = (df_cleaned['trip_miles'] == 0).sum()\n",
    "        # Only remove if significant - comment out if you want to keep zero-distance trips\n",
    "        # if zero_distance > 0:\n",
    "        #     print(f\"\\n[5] Removing zero-distance trips: {zero_distance:,} records\")\n",
    "        #     df_cleaned = df_cleaned[df_cleaned['trip_miles'] > 0]\n",
    "        #     cleaning_steps.append(f\"Removed {zero_distance:,} zero-distance trips\")\n",
    "    \n",
    "    # 6. Remove duplicate rows\n",
    "    duplicates = df_cleaned.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"\\n[6] Removing duplicate rows: {duplicates:,} records\")\n",
    "        df_cleaned = df_cleaned.drop_duplicates()\n",
    "        cleaning_steps.append(f\"Removed {duplicates:,} duplicate rows\")\n",
    "    else:\n",
    "        print(f\"\\n[6] [OK] No duplicate rows found\")\n",
    "    \n",
    "    # 7. Remove extremely long trips (optional - outliers)\n",
    "    if 'trip_time' in df_cleaned.columns:\n",
    "        # Remove trips longer than 24 hours\n",
    "        very_long = (df_cleaned['trip_time'] > 24 * 3600).sum()\n",
    "        if very_long > 0:\n",
    "            print(f\"\\n[7] Removing trips longer than 24 hours: {very_long:,} records\")\n",
    "            df_cleaned = df_cleaned[df_cleaned['trip_time'] <= 24 * 3600]\n",
    "            cleaning_steps.append(f\"Removed {very_long:,} trips longer than 24 hours\")\n",
    "        else:\n",
    "            print(f\"\\n[7] [OK] No trips longer than 24 hours found\")\n",
    "    else:\n",
    "        print(f\"\\n[7] [SKIP] trip_time column not found\")\n",
    "    \n",
    "    # 8. Remove extremely long distances (outliers)\n",
    "    if 'trip_miles' in df_cleaned.columns:\n",
    "        # Remove trips longer than 100 miles (likely errors)\n",
    "        extreme_miles = (df_cleaned['trip_miles'] > 100).sum()\n",
    "        if extreme_miles > 0:\n",
    "            print(f\"\\n[8] Removing trips longer than 100 miles: {extreme_miles:,} records\")\n",
    "            df_cleaned = df_cleaned[df_cleaned['trip_miles'] <= 100]\n",
    "            cleaning_steps.append(f\"Removed {extreme_miles:,} trips longer than 100 miles\")\n",
    "        else:\n",
    "            print(f\"\\n[8] [OK] No trips longer than 100 miles found\")\n",
    "    else:\n",
    "        print(f\"\\n[8] [SKIP] trip_miles column not found\")\n",
    "    \n",
    "    # 9. Remove extremely high fares (outliers)\n",
    "    if 'base_passenger_fare' in df_cleaned.columns:\n",
    "        # Remove trips with fare > $1000 (likely errors)\n",
    "        extreme_fare = (df_cleaned['base_passenger_fare'] > 1000).sum()\n",
    "        if extreme_fare > 0:\n",
    "            print(f\"\\n[9] Removing trips with fare > $1000: {extreme_fare:,} records\")\n",
    "            df_cleaned = df_cleaned[df_cleaned['base_passenger_fare'] <= 1000]\n",
    "            cleaning_steps.append(f\"Removed {extreme_fare:,} trips with fare > $1000\")\n",
    "        else:\n",
    "            print(f\"\\n[9] [OK] No trips with fare > $1000 found\")\n",
    "    else:\n",
    "        print(f\"\\n[9] [SKIP] base_passenger_fare column not found\")\n",
    "    \n",
    "    # Summary\n",
    "    cleaned_count = len(df_cleaned)\n",
    "    removed_count = original_count - cleaned_count\n",
    "    removal_rate = (removed_count / original_count * 100) if original_count > 0 else 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Cleaning Summary\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nOriginal records: {original_count:,}\")\n",
    "    print(f\"Cleaned records: {cleaned_count:,}\")\n",
    "    print(f\"Records removed: {removed_count:,} ({removal_rate:.6f}%)\")\n",
    "    print(f\"Records retained: {(cleaned_count/original_count*100):.6f}%\")\n",
    "    \n",
    "    if cleaning_steps:\n",
    "        print(f\"\\nCleaning steps performed:\")\n",
    "        for i, step in enumerate(cleaning_steps, 1):\n",
    "            print(f\"  {i}. {step}\")\n",
    "    \n",
    "    # Replace original dataframe with cleaned version\n",
    "    df = df_cleaned\n",
    "    print(f\"\\n[OK] Data cleaning complete! Using cleaned dataset with {len(df):,} records for further analysis.\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "else:\n",
    "    print(\"[ERROR] Please run the previous cell to load data first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Summary Statistics\n",
    "\n",
    "## Establish Baseline for Clean Data\n",
    "\n",
    "After cleaning, we establish baseline statistics for the clean dataset:\n",
    "\n",
    "- **Dataset structure**: Number of rows, columns, memory usage\n",
    "- **Temporal coverage**: Date range, number of days\n",
    "- **Location coverage**: Number of unique zones\n",
    "- **Numeric field statistics**: Mean, median, min, max for key fields\n",
    "- **Key metrics**: \n",
    "  - Average trip distance, duration, fare\n",
    "  - Trip distribution by distance/duration categories\n",
    "  - Peak patterns (hour, day, month)\n",
    "\n",
    "This provides a foundation for understanding the data before EDA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Summary Statistics - Clean Dataset\n",
      "================================================================================\n",
      "\n",
      "Dataset Overview:\n",
      "  Total records: 2,394,405\n",
      "  Columns: 11\n",
      "  Memory usage: ~200.946465 MB\n",
      "\n",
      "Temporal Coverage:\n",
      "  Date range: 2024-01-01 to 2024-12-31\n",
      "  Days covered: 365\n",
      "\n",
      "Location Coverage:\n",
      "  Unique pickup zones: 260\n",
      "  Unique dropoff zones: 261\n",
      "\n",
      "Key Metrics:\n",
      "  Average distance: 5.074432 miles (median: 3.012000)\n",
      "  Average duration: 20.119912 min (median: 16.300000 min)\n",
      "  Average fare: $26.192363 (median: $19.610000)\n",
      "\n",
      "Sampling Information:\n",
      "  Sampling ratio: 1.000000%\n",
      "  Estimated total records: 239,440,500.000000\n",
      "\n",
      "================================================================================\n",
      "Summary statistics complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary Statistics for Clean Data\n",
    "if 'df' in locals() and df is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Summary Statistics - Clean Dataset\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Dataset Overview\n",
    "    print(f\"\\nDataset Overview:\")\n",
    "    print(f\"  Total records: {len(df):,}\")\n",
    "    print(f\"  Columns: {len(df.columns)}\")\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    print(f\"  Memory usage: ~{memory_mb:.6f} MB\")\n",
    "    \n",
    "    # Temporal coverage\n",
    "    if 'pickup_datetime' in df.columns:\n",
    "        print(f\"\\nTemporal Coverage:\")\n",
    "        print(f\"  Date range: {df['pickup_datetime'].min().date()} to {df['pickup_datetime'].max().date()}\")\n",
    "        span_days = (df['pickup_datetime'].max() - df['pickup_datetime'].min()).days\n",
    "        print(f\"  Days covered: {span_days}\")\n",
    "    \n",
    "    # Location coverage\n",
    "    if 'PULocationID' in df.columns:\n",
    "        print(f\"\\nLocation Coverage:\")\n",
    "        print(f\"  Unique pickup zones: {df['PULocationID'].nunique()}\")\n",
    "        print(f\"  Unique dropoff zones: {df['DOLocationID'].nunique()}\")\n",
    "    \n",
    "    # Key metrics\n",
    "    print(f\"\\nKey Metrics:\")\n",
    "    if 'trip_miles' in df.columns:\n",
    "        print(f\"  Average distance: {df['trip_miles'].mean():.6f} miles (median: {df['trip_miles'].median():.6f})\")\n",
    "    if 'trip_time' in df.columns:\n",
    "        print(f\"  Average duration: {df['trip_time'].mean()/60:.6f} min (median: {df['trip_time'].median()/60:.6f} min)\")\n",
    "    if 'base_passenger_fare' in df.columns:\n",
    "        print(f\"  Average fare: ${df['base_passenger_fare'].mean():.6f} (median: ${df['base_passenger_fare'].median():.6f})\")\n",
    "    \n",
    "    # Sampling info\n",
    "    if 'SAMPLE_RATIO' in locals():\n",
    "        print(f\"\\nSampling Information:\")\n",
    "        print(f\"  Sampling ratio: {SAMPLE_RATIO*100:.6f}%\")\n",
    "        print(f\"  Estimated total records: {len(df)/SAMPLE_RATIO:,.6f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Summary statistics complete!\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"[ERROR] Please run the data cleaning cell first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "## Discover Insights\n",
    "\n",
    "This is the main analysis section where we explore the data to discover insights through visualizations and statistical analysis.\n",
    "\n",
    "### 5.1 Distribution Analysis\n",
    "- Trip distance distribution\n",
    "- Trip duration distribution\n",
    "- Fare distribution\n",
    "\n",
    "### 5.2 Temporal Analysis\n",
    "- Hourly patterns (trips by hour of day)\n",
    "- Daily patterns (trips by day of week)\n",
    "- Weekday vs Weekend patterns\n",
    "\n",
    "### 5.3 Location Analysis\n",
    "- Top pickup locations\n",
    "- Top dropoff locations\n",
    "- Location-based insights\n",
    "\n",
    "### 5.4 Demand Analysis\n",
    "- Demand heatmap (hour × zone)\n",
    "- Peak hour analysis\n",
    "- Demand volatility analysis\n",
    "\n",
    "### 5.5 Outlier Analysis\n",
    "- Identify and visualize outliers in key fields\n",
    "\n",
    "### 5.6 Interactive Visualizations\n",
    "- NYC hourly demand map (interactive HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Generating visualization charts...\n",
      "================================================================================\n",
      "\n",
      "[1/7] Creating distribution plots...\n",
      "\n",
      "Creating distribution plots...\n",
      "  [OK] Saved: trip_distance_distribution.png\n",
      "  [OK] Saved: trip_duration_distribution.png\n",
      "  [OK] Saved: fare_distribution.png\n",
      "\n",
      "[2/7] Creating temporal analysis...\n",
      "\n",
      "Creating temporal analysis...\n",
      "  [OK] Saved: trips_by_hour.png\n",
      "  [OK] Saved: trips_by_day.png\n",
      "\n",
      "[3/7] Creating location analysis...\n",
      "\n",
      "Creating location analysis...\n",
      "  [OK] Saved: top_pickup_locations.png\n",
      "  [OK] Saved: top_dropoff_locations.png\n",
      "\n",
      "[4/7] Creating demand analysis...\n",
      "\n",
      "Creating demand analysis outputs...\n",
      "  [OK] Saved: demand_heatmap.png\n",
      "  [OK] Saved: peak_hour_chart.png\n",
      "  [OK] Saved: temporal_pattern_plot.png\n",
      "  [OK] Saved: demand_volatility_table.png\n",
      "  [OK] Saved: demand_volatility_table.csv -> plots\\demand_volatility_table.csv\n",
      "\n",
      "[6/7] Creating NYC interactive map...\n",
      "\n",
      "Creating interactive NYC demand map...\n",
      "  [OK] Saved: nyc_hourly_interactive_map.html -> plots\\nyc_hourly_interactive_map.html\n",
      "\n",
      "[7/7] Creating outlier analysis...\n",
      "\n",
      "Creating outlier analysis...\n",
      "  [OK] Saved: outlier_boxplots.png\n",
      "\n",
      "================================================================================\n",
      "Visualization complete!\n",
      "================================================================================\n",
      "All charts saved to 'plots/'  directory\n"
     ]
    }
   ],
   "source": [
    "if 'df' in locals() and df is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Generating visualization charts...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n[1/7] Creating distribution plots...\")\n",
    "    create_distribution_plots(df)\n",
    "    \n",
    "    print(f\"\\n[2/7] Creating temporal analysis...\")\n",
    "    create_temporal_analysis(df)\n",
    "    \n",
    "    print(f\"\\n[3/7] Creating location analysis...\")\n",
    "    create_location_analysis(df, zone_lookup_dict=zone_lookup_dict)\n",
    "    \n",
    "    print(f\"\\n[4/7] Creating demand analysis...\")\n",
    "    create_demand_analysis(df, zone_lookup_dict=zone_lookup_dict, sample_ratio=SAMPLE_RATIO)\n",
    "    \n",
    "    print(f\"\\n[6/7] Creating NYC interactive map...\")\n",
    "    create_manhattan_interactive_map(df)\n",
    "    \n",
    "    print(f\"\\n[7/7] Creating outlier analysis...\")\n",
    "    create_outlier_analysis(df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Visualization complete!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"All charts saved to 'plots/'  directory\")\n",
    "else:\n",
    "    print(\"[ERROR] Please run the previous cell to load data\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
